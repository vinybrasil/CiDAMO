{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ANTON_apresentacao.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "86d49d9cdbce4c7aada9d9aa30cf957b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_da9499ce03c6474ca31725afd934816f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_99a1100f2c6e4371b10911bafddbe1bb",
              "IPY_MODEL_40ee5919002c42c3bbd1e2dd23d648d8"
            ]
          }
        },
        "da9499ce03c6474ca31725afd934816f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "99a1100f2c6e4371b10911bafddbe1bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_b6148c2412fe4c84a2c63c4110208f20",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 871891,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 871891,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9feba11f227b4267b0ea574c62f55d32"
          }
        },
        "40ee5919002c42c3bbd1e2dd23d648d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6872772991414a10861d85a756cd37b3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 872k/872k [00:00&lt;00:00, 1.67MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_048030f5e8184b27973741795489db94"
          }
        },
        "b6148c2412fe4c84a2c63c4110208f20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9feba11f227b4267b0ea574c62f55d32": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6872772991414a10861d85a756cd37b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "048030f5e8184b27973741795489db94": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "SY7SbhRqVu6g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d14962f3-5f72-4977-c7a2-0897a9a508b8"
      },
      "source": [
        "import os\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%tensorflow_version 1.x\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZp14VMSV1xl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "6e15c4fe-2396-415e-ce24-cb5f514dab95"
      },
      "source": [
        "data = pd.read_csv(\"/content/drive/My Drive/BERT/ANTON/final.csv\")\n",
        "data = data.drop(\"Unnamed: 0\", axis=1)\n",
        "data.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review_rate</th>\n",
              "      <th>review_body</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>10</td>\n",
              "      <td>Pois bem...as fotos dos pratos, bebidas e doce...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>30</td>\n",
              "      <td>Espero que utilizem essa avaliação para rever ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>20</td>\n",
              "      <td>Fomos comer a sobremesa as 20h40, sentamos, pe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>10</td>\n",
              "      <td>Pedimos o cardápio e ao chamar o atendente ped...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>50</td>\n",
              "      <td>Fui com algumas amigas em uma segunda-feira, d...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   review_rate                                        review_body\n",
              "0           10  Pois bem...as fotos dos pratos, bebidas e doce...\n",
              "1           30  Espero que utilizem essa avaliação para rever ...\n",
              "2           20  Fomos comer a sobremesa as 20h40, sentamos, pe...\n",
              "3           10  Pedimos o cardápio e ao chamar o atendente ped...\n",
              "4           50  Fui com algumas amigas em uma segunda-feira, d..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-qOsVlSV7Sb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "4e12d46a-d90f-4dd9-a672-ce1951c39cc3"
      },
      "source": [
        "def DATA_BINARIZADOR_RESULTADOS(data):\n",
        "    RESULTADO_BINARIO = []\n",
        "    \n",
        "    for item in data['review_rate']:\n",
        "        if item < 40 :\n",
        "            RESULTADO_BINARIO.append(0)\n",
        "        elif item >= 40:\n",
        "            RESULTADO_BINARIO.append(1)\n",
        "        else:\n",
        "            print(\"algo deu ruim\")\n",
        "            \n",
        "    \n",
        "    data['review_rate'] = RESULTADO_BINARIO\n",
        "    return data\n",
        "data = DATA_BINARIZADOR_RESULTADOS(data)\n",
        "data.head(10)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review_rate</th>\n",
              "      <th>review_body</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Pois bem...as fotos dos pratos, bebidas e doce...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>Espero que utilizem essa avaliação para rever ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>Fomos comer a sobremesa as 20h40, sentamos, pe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>Pedimos o cardápio e ao chamar o atendente ped...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>Fui com algumas amigas em uma segunda-feira, d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1</td>\n",
              "      <td>Quem gosta de doce, é um prato cheio. Erramos ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0</td>\n",
              "      <td>Atendimento muito bom, confesso que quase peca...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0</td>\n",
              "      <td>As sobremesas são realmente uma delícia, porém...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1</td>\n",
              "      <td>O bar oferece diversas opções de comidinhas de...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1</td>\n",
              "      <td>Ao chegar, o local já agrada. Tanto o visual, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   review_rate                                        review_body\n",
              "0            0  Pois bem...as fotos dos pratos, bebidas e doce...\n",
              "1            0  Espero que utilizem essa avaliação para rever ...\n",
              "2            0  Fomos comer a sobremesa as 20h40, sentamos, pe...\n",
              "3            0  Pedimos o cardápio e ao chamar o atendente ped...\n",
              "4            1  Fui com algumas amigas em uma segunda-feira, d...\n",
              "5            1  Quem gosta de doce, é um prato cheio. Erramos ...\n",
              "6            0  Atendimento muito bom, confesso que quase peca...\n",
              "7            0  As sobremesas são realmente uma delícia, porém...\n",
              "8            1  O bar oferece diversas opções de comidinhas de...\n",
              "9            1  Ao chegar, o local já agrada. Tanto o visual, ..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzZIYT0mWCwp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = data.review_body.values\n",
        "y = data.review_rate.values\n",
        "\n",
        "X_train, X_val, y_train, y_val =\\\n",
        "    train_test_split(X, y, test_size=0.1, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n76_zFxvWGrD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "08dbe2a5-b583-4562-cd4e-4f9f03b2818e"
      },
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():       \n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f'Há {torch.cuda.device_count()} GPU(s) disponivel.')\n",
        "    print('Nome:', torch.cuda.get_device_name(0))\n",
        "\n",
        "else:\n",
        "    print('Sem GPU, usando a CPU.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Há 1 GPU(s) disponivel.\n",
            "Nome: Tesla P100-PCIE-16GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEGCMDSnWIkw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import accuracy_score, roc_curve, auc\n",
        "\n",
        "def evaluate_roc(probs, y_true):\n",
        "  \n",
        "    preds = probs[:, 1]\n",
        "    fpr, tpr, threshold = roc_curve(y_true, preds)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    print(f'AUC: {roc_auc:.4f}')\n",
        "       \n",
        " \n",
        "    y_pred = np.where(preds >= 0.5, 1, 0)\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    print(f'Acurácia: {accuracy*100:.2f}%')\n",
        "    \n",
        "    # Plotando a ROC\n",
        "    plt.title('Curva ROC')\n",
        "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
        "    plt.legend(loc = 'lower right')\n",
        "    plt.plot([0, 1], [0, 1],'r--')\n",
        "    plt.xlim([0, 1])\n",
        "    plt.ylim([0, 1])\n",
        "    plt.ylabel('Taxa de true positive')\n",
        "    plt.xlabel('Taxa de false positive ')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9D2mQKKOWgsA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 735
        },
        "outputId": "bbff1e12-2533-484a-ff85-b377b05f3724"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/78/92cedda05552398352ed9784908b834ee32a0bd071a9b32de287327370b7/transformers-2.8.0-py3-none-any.whl (563kB)\n",
            "\r\u001b[K     |▋                               | 10kB 28.1MB/s eta 0:00:01\r\u001b[K     |█▏                              | 20kB 6.1MB/s eta 0:00:01\r\u001b[K     |█▊                              | 30kB 8.6MB/s eta 0:00:01\r\u001b[K     |██▎                             | 40kB 11.0MB/s eta 0:00:01\r\u001b[K     |███                             | 51kB 6.4MB/s eta 0:00:01\r\u001b[K     |███▌                            | 61kB 7.5MB/s eta 0:00:01\r\u001b[K     |████                            | 71kB 8.5MB/s eta 0:00:01\r\u001b[K     |████▋                           | 81kB 9.5MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 92kB 7.8MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 102kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 112kB 8.6MB/s eta 0:00:01\r\u001b[K     |███████                         | 122kB 8.6MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 133kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 143kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 153kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 163kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 174kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 184kB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████                     | 194kB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 204kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 215kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 225kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 235kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 245kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 256kB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 266kB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 276kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 286kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 296kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 307kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 317kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 327kB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 337kB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 348kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 358kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 368kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 378kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 389kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 399kB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 409kB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 419kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 430kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 440kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 450kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 460kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 471kB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 481kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 491kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 501kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 512kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 522kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 532kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 542kB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 552kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 563kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 573kB 8.6MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/50/93509f906a40bffd7d175f97fd75ea328ad9bd91f48f59c4bd084c94a25e/sacremoses-0.0.41.tar.gz (883kB)\n",
            "\r\u001b[K     |▍                               | 10kB 26.6MB/s eta 0:00:01\r\u001b[K     |▊                               | 20kB 33.2MB/s eta 0:00:01\r\u001b[K     |█▏                              | 30kB 40.0MB/s eta 0:00:01\r\u001b[K     |█▌                              | 40kB 44.5MB/s eta 0:00:01\r\u001b[K     |█▉                              | 51kB 47.1MB/s eta 0:00:01\r\u001b[K     |██▎                             | 61kB 50.2MB/s eta 0:00:01\r\u001b[K     |██▋                             | 71kB 51.7MB/s eta 0:00:01\r\u001b[K     |███                             | 81kB 52.5MB/s eta 0:00:01\r\u001b[K     |███▍                            | 92kB 53.7MB/s eta 0:00:01\r\u001b[K     |███▊                            | 102kB 54.9MB/s eta 0:00:01\r\u001b[K     |████                            | 112kB 54.9MB/s eta 0:00:01\r\u001b[K     |████▌                           | 122kB 54.9MB/s eta 0:00:01\r\u001b[K     |████▉                           | 133kB 54.9MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 143kB 54.9MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 153kB 54.9MB/s eta 0:00:01\r\u001b[K     |██████                          | 163kB 54.9MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 174kB 54.9MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 184kB 54.9MB/s eta 0:00:01\r\u001b[K     |███████                         | 194kB 54.9MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 204kB 54.9MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 215kB 54.9MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 225kB 54.9MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 235kB 54.9MB/s eta 0:00:01\r\u001b[K     |█████████                       | 245kB 54.9MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 256kB 54.9MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 266kB 54.9MB/s eta 0:00:01\r\u001b[K     |██████████                      | 276kB 54.9MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 286kB 54.9MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 296kB 54.9MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 307kB 54.9MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 317kB 54.9MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 327kB 54.9MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 337kB 54.9MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 348kB 54.9MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 358kB 54.9MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 368kB 54.9MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 378kB 54.9MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 389kB 54.9MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 399kB 54.9MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 409kB 54.9MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 419kB 54.9MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 430kB 54.9MB/s eta 0:00:01\r\u001b[K     |████████████████                | 440kB 54.9MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 450kB 54.9MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 460kB 54.9MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 471kB 54.9MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 481kB 54.9MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 491kB 54.9MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 501kB 54.9MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 512kB 54.9MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 522kB 54.9MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 532kB 54.9MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 542kB 54.9MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 552kB 54.9MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 563kB 54.9MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 573kB 54.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 583kB 54.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 593kB 54.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 604kB 54.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 614kB 54.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 624kB 54.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 634kB 54.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 645kB 54.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 655kB 54.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 665kB 54.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 675kB 54.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 686kB 54.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 696kB 54.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 706kB 54.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 716kB 54.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 727kB 54.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 737kB 54.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 747kB 54.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 757kB 54.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 768kB 54.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 778kB 54.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 788kB 54.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 798kB 54.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 808kB 54.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 819kB 54.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 829kB 54.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 839kB 54.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 849kB 54.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 860kB 54.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 870kB 54.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 880kB 54.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 890kB 54.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.2)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 48.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.40)\n",
            "Collecting tokenizers==0.5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7MB 32.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.40 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.40)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.40->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.40->boto3->transformers) (2.8.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.41-cp36-none-any.whl size=893334 sha256=3031d1046c45f5f068282206c4dcecccba7f4e10c1e410ae45efc8ac6708ce21\n",
            "  Stored in directory: /root/.cache/pip/wheels/22/5a/d4/b020a81249de7dc63758a34222feaa668dbe8ebfe9170cc9b1\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, sentencepiece, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.41 sentencepiece-0.1.85 tokenizers-0.5.2 transformers-2.8.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_ZlUIpNWhbR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "86d49d9cdbce4c7aada9d9aa30cf957b",
            "da9499ce03c6474ca31725afd934816f",
            "99a1100f2c6e4371b10911bafddbe1bb",
            "40ee5919002c42c3bbd1e2dd23d648d8",
            "b6148c2412fe4c84a2c63c4110208f20",
            "9feba11f227b4267b0ea574c62f55d32",
            "6872772991414a10861d85a756cd37b3",
            "048030f5e8184b27973741795489db94"
          ]
        },
        "outputId": "0179a012-3ef9-45dc-eeec-4af3e82e9fdf"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased', do_lower_case=True)\n",
        "\n",
        "# Create a function to tokenize a set of texts\n",
        "def preprocessing_for_bert(data):\n",
        "    \"\"\"Perform required preprocessing steps for pretrained BERT.\n",
        "    @param    data (np.array): Array of texts to be processed.\n",
        "    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n",
        "    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n",
        "                  tokens should be attended to by the model.\n",
        "    \"\"\"\n",
        "    # Create empty lists to store outputs\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "\n",
        "    # For every sentence...\n",
        "    for sent in data:\n",
        "        # `encode_plus` will:\n",
        "        #    (1) Tokenize the sentence\n",
        "        #    (2) Add the `[CLS]` and `[SEP]` token to the start and end\n",
        "        #    (3) Truncate/Pad sentence to max length\n",
        "        #    (4) Map tokens to their IDs\n",
        "        #    (5) Create attention mask\n",
        "        #    (6) Return a dictionary of outputs\n",
        "        encoded_sent = tokenizer.encode_plus(\n",
        "            text=text_preprocessing(sent),  # Preprocess sentence\n",
        "            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n",
        "            max_length=MAX_LEN,                  # Max length to truncate/pad\n",
        "            pad_to_max_length=True,         # Pad sentence to max length\n",
        "            #return_tensors='pt',           # Return PyTorch tensor\n",
        "            return_attention_mask=True      # Return attention mask\n",
        "            )\n",
        "        \n",
        "        # Add the outputs to the lists\n",
        "        input_ids.append(encoded_sent.get('input_ids'))\n",
        "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
        "\n",
        "    # Convert lists to tensors\n",
        "    input_ids = torch.tensor(input_ids)\n",
        "    attention_masks = torch.tensor(attention_masks)\n",
        "\n",
        "    return input_ids, attention_masks"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "86d49d9cdbce4c7aada9d9aa30cf957b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=871891, style=ProgressStyle(description_wid…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGwrytQaWm49",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "90966420-a8e6-4e1b-c7a3-3ddfe216cecd"
      },
      "source": [
        "encoded_tweets = [tokenizer.encode(sent, add_special_tokens=True) for sent in data.review_body]\n",
        "max_len = max([len(sent) for sent in encoded_tweets])\n",
        "print('Max length: ', max_len)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1191 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (529 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (830 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (696 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (784 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (529 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (529 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (550 > 512). Running this sequence through the model will result in indexing errors\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Max length:  1193\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xBXer8SW4jE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def text_preprocessing(text):\n",
        "   \n",
        " \n",
        "    # Remove '@name'\n",
        "    text = re.sub(r'(@.*?)[\\s]', ' ', text)\n",
        "\n",
        "    # Replace '&amp;' with '&'\n",
        "    text = re.sub(r'&amp;', '&', text)\n",
        "\n",
        "    # Remove trailing whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBPNR6_EWtvV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "outputId": "08391058-fb73-48f0-f22e-64e3d2c09284"
      },
      "source": [
        "MAX_LEN = 64\n",
        "\n",
        "token_ids = list(preprocessing_for_bert([X[0]])[0].squeeze().numpy())\n",
        "print('Original: ', X[0])\n",
        "print('Token IDs: ', token_ids)\n",
        "\n",
        "\n",
        "print('Tokenizando os dados...')\n",
        "train_inputs, train_masks = preprocessing_for_bert(X_train)\n",
        "val_inputs, val_masks = preprocessing_for_bert(X_val)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:  Pois bem...as fotos dos pratos, bebidas e doces são maravilhosas...daquelas q vc se empolga e diz: vale a pena conhecer.. porém tudo acaba quando vc entra...infelizmente.Cheguei com meu marido e sentamos.. mesa suja..mas ficamos aguardando o atendimento...5 min., 10 min., (o atendente chegou na mesa ao lado, tirou o pedido e nada de nos atender.Outro atendente serviu a outra mesa ao lado..13 minutos. Apertamos o botão pra acionar o atendente (aquele q fica na mesa) e continuamos aguardando..15 min., 20 min.(e a gente só de olho...)..a mesa ainda suja.Finalmente passados 25 minutos apareceu a atendente Tainá e ao explicarmos o ocorrido, ficava mais olhando a mesa ao lado do que nos ouvindo..teve a coragem ainda de interromper o que estávamos falando pra perguntar pra mesa ao lado se estava tudo ok com eles!tipo: conversa com a minha mão! Nessas eu disse: chega! Ela nem está ouvindo o que estamos falando ( q era: olha vcs devem estar correndo aí.. agora não da tempo de pedirmos nada pois temos um compromisso e precisamos ir...)Resumo: saímos sem comer mas com a certeza de que nesse lugar falta muita coisa...treinamento e respeito! Saímos de lá indignados e fiz um comentário no Instagram marcando o bar..e não é q eles desativaram o meu comentário e nem entraram em contato comigo pra saber o q tinha acontecido?? Ficou pior...bem pior..além de pior ficou feio...uma decepção... (Detalhe q o gerente estava o tempo todo preparando as bebidas e ninguém de olho no salao, tipo abandonado mesmo..por isso nem chamamos pra reclamar, caso contrário poderiam perder mais clientes.. uma pena...)\n",
            "Token IDs:  [101, 20518, 19060, 119, 119, 119, 10146, 45588, 10426, 62499, 10107, 117, 10346, 74829, 147, 35849, 10107, 11132, 25767, 20386, 93053, 10107, 119, 119, 119, 10141, 56899, 10107, 159, 63632, 10128, 10252, 32014, 10547, 147, 34026, 131, 23172, 143, 20685, 10173, 11448, 16951, 119, 119, 24569, 38434, 36235, 11798, 63632, 25937, 119, 119, 119, 56551, 16180, 28958, 14425, 119, 10283, 17734, 10116, 10241, 102]\n",
            "Tokenizando os dados...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oALtF17pWyB7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#CRIANDO O DATALOADER:\n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "train_labels = torch.tensor(y_train)\n",
        "val_labels = torch.tensor(y_val)\n",
        "\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltFTyh8eW_Rr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "84a00706-fc88-4a98-9962-c5a32e37766b"
      },
      "source": [
        "%%time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertModel\n",
        "\n",
        "\n",
        "class BertClassifier(nn.Module):\n",
        "   \n",
        "    def __init__(self, freeze_bert=False):\n",
        "     \n",
        "        super(BertClassifier, self).__init__()\n",
        "        D_in, H, D_out = 768, 50, 2\n",
        "\n",
        "    \n",
        "        self.bert = BertModel.from_pretrained('bert-base-multilingual-uncased')\n",
        "\n",
        "     \n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(D_in, H),\n",
        "            nn.ReLU(),\n",
        "         \n",
        "            nn.Linear(H, D_out)\n",
        "        )\n",
        "\n",
        "      \n",
        "        if freeze_bert:\n",
        "            for param in self.bert.parameters():\n",
        "                param.requires_grad = False\n",
        "        \n",
        "    def forward(self, input_ids, attention_mask):\n",
        "         \n",
        "        outputs = self.bert(input_ids=input_ids,\n",
        "                            attention_mask=attention_mask)\n",
        "        \n",
        "      \n",
        "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
        "\n",
        "        logits = self.classifier(last_hidden_state_cls)\n",
        "\n",
        "        return logits"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 45 µs, sys: 0 ns, total: 45 µs\n",
            "Wall time: 47.4 µs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRMd5BdxXCQ8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "def initialize_model(epochs=4):\n",
        "   \n",
        "    \n",
        "    bert_classifier = BertClassifier(freeze_bert=False)\n",
        "\n",
        "    \n",
        "    bert_classifier.to(device)\n",
        "\n",
        "    # Create the optimizer\n",
        "    optimizer = AdamW(bert_classifier.parameters(),\n",
        "                      lr=5e-5,   \n",
        "                      eps=1e-8  \n",
        "                      )\n",
        "\n",
        "    \n",
        "    total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "    \n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                                num_warmup_steps=0, \n",
        "                                                num_training_steps=total_steps)\n",
        "    return bert_classifier, optimizer, scheduler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFDmMD6CXFJz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import time\n",
        "\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "def set_seed(seed_value=42):\n",
        "    \n",
        "    random.seed(seed_value)\n",
        "    np.random.seed(seed_value)\n",
        "    torch.manual_seed(seed_value)\n",
        "    torch.cuda.manual_seed_all(seed_value)\n",
        "\n",
        "def train(model, train_dataloader, val_dataloader=None, epochs=4, evaluation=False):\n",
        "   \n",
        "    print(\"Treino iniciado...\\n\")\n",
        "    for epoch_i in range(epochs):\n",
        "\n",
        "        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val. Acuracia':^9} | {'Tempo':^9}\")\n",
        "        print(\"-\"*70)\n",
        "\n",
        "        t0_epoch, t0_batch = time.time(), time.time()\n",
        "\n",
        "       \n",
        "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
        "\n",
        "        \n",
        "        model.train()\n",
        "\n",
        "        \n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            batch_counts +=1\n",
        "            \n",
        "            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "            \n",
        "            model.zero_grad()\n",
        "\n",
        "           \n",
        "            logits = model(b_input_ids, b_attn_mask)\n",
        "\n",
        "          \n",
        "            loss = loss_fn(logits, b_labels)\n",
        "            batch_loss += loss.item()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "          \n",
        "            loss.backward()\n",
        "\n",
        "        \n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "          \n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "      \n",
        "            if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
        "              \n",
        "                time_elapsed = time.time() - t0_batch\n",
        "\n",
        "                \n",
        "                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n",
        "\n",
        "              \n",
        "                batch_loss, batch_counts = 0, 0\n",
        "                t0_batch = time.time()\n",
        "\n",
        "       \n",
        "        avg_train_loss = total_loss / len(train_dataloader)\n",
        "\n",
        "        print(\"-\"*70)\n",
        "  \n",
        "        if evaluation == True:\n",
        "           \n",
        "            val_loss, val_accuracy = evaluate(model, val_dataloader)\n",
        "\n",
        "         \n",
        "            time_elapsed = time.time() - t0_epoch\n",
        "            \n",
        "            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n",
        "            print(\"-\"*70)\n",
        "        print(\"\\n\")\n",
        "    \n",
        "    print(\"Fim da etapa de treinamento!\")\n",
        "\n",
        "\n",
        "def evaluate(model, val_dataloader):\n",
        "   \n",
        "    model.eval()\n",
        "\n",
        "\n",
        "    val_accuracy = []\n",
        "    val_loss = []\n",
        "\n",
        "    for batch in val_dataloader:\n",
        "       \n",
        "        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "     \n",
        "        with torch.no_grad():\n",
        "            logits = model(b_input_ids, b_attn_mask)\n",
        "\n",
        "       \n",
        "        loss = loss_fn(logits, b_labels)\n",
        "        val_loss.append(loss.item())\n",
        "\n",
        "        \n",
        "        preds = torch.argmax(logits, dim=1).flatten()\n",
        "\n",
        "       \n",
        "        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
        "        val_accuracy.append(accuracy)\n",
        "\n",
        "   \n",
        "    val_loss = np.mean(val_loss)\n",
        "    val_accuracy = np.mean(val_accuracy)\n",
        "\n",
        "    return val_loss, val_accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zG5e4vlUXJBB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "de39f3ba-09cb-4572-d7f6-10ccfa9db0c7"
      },
      "source": [
        "set_seed(42)    \n",
        "bert_classifier, optimizer, scheduler = initialize_model(epochs=2)\n",
        "train(bert_classifier, train_dataloader, val_dataloader, epochs=50, evaluation=True)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Treino iniciado...\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  | Val. Acuracia |   Tempo  \n",
            "----------------------------------------------------------------------\n",
            "   1    |   20    |   0.391728   |     -      |     -     |   4.68   \n",
            "   1    |   40    |   0.342025   |     -      |     -     |   4.46   \n",
            "   1    |   60    |   0.350529   |     -      |     -     |   4.45   \n",
            "   1    |   80    |   0.307925   |     -      |     -     |   4.47   \n",
            "   1    |   100   |   0.294726   |     -      |     -     |   4.48   \n",
            "   1    |   120   |   0.294523   |     -      |     -     |   4.48   \n",
            "   1    |   140   |   0.297069   |     -      |     -     |   4.49   \n",
            "   1    |   160   |   0.272421   |     -      |     -     |   4.47   \n",
            "   1    |   180   |   0.217362   |     -      |     -     |   4.48   \n",
            "   1    |   200   |   0.338763   |     -      |     -     |   4.48   \n",
            "   1    |   215   |   0.242705   |     -      |     -     |   3.32   \n",
            "----------------------------------------------------------------------\n",
            "   1    |    -    |   0.306360   |  0.261501  |   90.62   |   49.69  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  | Val. Acuracia |   Tempo  \n",
            "----------------------------------------------------------------------\n",
            "   2    |   20    |   0.238907   |     -      |     -     |   4.68   \n",
            "   2    |   40    |   0.219217   |     -      |     -     |   4.47   \n",
            "   2    |   60    |   0.231911   |     -      |     -     |   4.48   \n",
            "   2    |   80    |   0.261587   |     -      |     -     |   4.47   \n",
            "   2    |   100   |   0.220764   |     -      |     -     |   4.50   \n",
            "   2    |   120   |   0.233979   |     -      |     -     |   4.47   \n",
            "   2    |   140   |   0.206399   |     -      |     -     |   4.48   \n",
            "   2    |   160   |   0.224827   |     -      |     -     |   4.48   \n",
            "   2    |   180   |   0.208968   |     -      |     -     |   4.48   \n",
            "   2    |   200   |   0.189171   |     -      |     -     |   4.49   \n",
            "   2    |   215   |   0.231271   |     -      |     -     |   3.32   \n",
            "----------------------------------------------------------------------\n",
            "   2    |    -    |   0.224178   |  0.235869  |   91.28   |   49.74  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  | Val. Acuracia |   Tempo  \n",
            "----------------------------------------------------------------------\n",
            "   3    |   20    |   0.190197   |     -      |     -     |   4.69   \n",
            "   3    |   40    |   0.183476   |     -      |     -     |   4.47   \n",
            "   3    |   60    |   0.179116   |     -      |     -     |   4.47   \n",
            "   3    |   80    |   0.191932   |     -      |     -     |   4.48   \n",
            "   3    |   100   |   0.201387   |     -      |     -     |   4.50   \n",
            "   3    |   120   |   0.183198   |     -      |     -     |   4.49   \n",
            "   3    |   140   |   0.205559   |     -      |     -     |   4.48   \n",
            "   3    |   160   |   0.207338   |     -      |     -     |   4.48   \n",
            "   3    |   180   |   0.173948   |     -      |     -     |   4.47   \n",
            "   3    |   200   |   0.218891   |     -      |     -     |   4.47   \n",
            "   3    |   215   |   0.191689   |     -      |     -     |   3.32   \n",
            "----------------------------------------------------------------------\n",
            "   3    |    -    |   0.193363   |  0.235869  |   91.28   |   49.76  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  | Val. Acuracia |   Tempo  \n",
            "----------------------------------------------------------------------\n",
            "   4    |   20    |   0.220642   |     -      |     -     |   4.69   \n",
            "   4    |   40    |   0.193815   |     -      |     -     |   4.48   \n",
            "   4    |   60    |   0.207618   |     -      |     -     |   4.48   \n",
            "   4    |   80    |   0.161363   |     -      |     -     |   4.47   \n",
            "   4    |   100   |   0.215025   |     -      |     -     |   4.47   \n",
            "   4    |   120   |   0.172645   |     -      |     -     |   4.47   \n",
            "   4    |   140   |   0.178371   |     -      |     -     |   4.47   \n",
            "   4    |   160   |   0.173379   |     -      |     -     |   4.48   \n",
            "   4    |   180   |   0.176974   |     -      |     -     |   4.48   \n",
            "   4    |   200   |   0.211715   |     -      |     -     |   4.49   \n",
            "   4    |   215   |   0.224605   |     -      |     -     |   3.32   \n",
            "----------------------------------------------------------------------\n",
            "   4    |    -    |   0.193614   |  0.235869  |   91.28   |   49.74  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  | Val. Acuracia |   Tempo  \n",
            "----------------------------------------------------------------------\n",
            "   5    |   20    |   0.188694   |     -      |     -     |   4.69   \n",
            "   5    |   40    |   0.189182   |     -      |     -     |   4.48   \n",
            "   5    |   60    |   0.148858   |     -      |     -     |   4.48   \n",
            "   5    |   80    |   0.211276   |     -      |     -     |   4.47   \n",
            "   5    |   100   |   0.179821   |     -      |     -     |   4.48   \n",
            "   5    |   120   |   0.194988   |     -      |     -     |   4.48   \n",
            "   5    |   140   |   0.223791   |     -      |     -     |   4.48   \n",
            "   5    |   160   |   0.189564   |     -      |     -     |   4.48   \n",
            "   5    |   180   |   0.204107   |     -      |     -     |   4.48   \n",
            "   5    |   200   |   0.213647   |     -      |     -     |   4.48   \n",
            "   5    |   215   |   0.171840   |     -      |     -     |   3.32   \n",
            "----------------------------------------------------------------------\n",
            "   5    |    -    |   0.192800   |  0.235869  |   91.28   |   49.75  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  | Val. Acuracia |   Tempo  \n",
            "----------------------------------------------------------------------\n",
            "   6    |   20    |   0.203535   |     -      |     -     |   4.69   \n",
            "   6    |   40    |   0.194289   |     -      |     -     |   4.49   \n",
            "   6    |   60    |   0.237317   |     -      |     -     |   4.48   \n",
            "   6    |   80    |   0.191417   |     -      |     -     |   4.48   \n",
            "   6    |   100   |   0.204924   |     -      |     -     |   4.49   \n",
            "   6    |   120   |   0.168721   |     -      |     -     |   4.47   \n",
            "   6    |   140   |   0.192341   |     -      |     -     |   4.49   \n",
            "   6    |   160   |   0.176015   |     -      |     -     |   4.49   \n",
            "   6    |   180   |   0.173874   |     -      |     -     |   4.48   \n",
            "   6    |   200   |   0.167863   |     -      |     -     |   4.48   \n",
            "   6    |   215   |   0.229181   |     -      |     -     |   3.32   \n",
            "----------------------------------------------------------------------\n",
            "   6    |    -    |   0.193737   |  0.235869  |   91.28   |   49.78  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  | Val. Acuracia |   Tempo  \n",
            "----------------------------------------------------------------------\n",
            "   7    |   20    |   0.206159   |     -      |     -     |   4.70   \n",
            "   7    |   40    |   0.169717   |     -      |     -     |   4.48   \n",
            "   7    |   60    |   0.222918   |     -      |     -     |   4.48   \n",
            "   7    |   80    |   0.182142   |     -      |     -     |   4.49   \n",
            "   7    |   100   |   0.219253   |     -      |     -     |   4.49   \n",
            "   7    |   120   |   0.225474   |     -      |     -     |   4.49   \n",
            "   7    |   140   |   0.184137   |     -      |     -     |   4.48   \n",
            "   7    |   160   |   0.180633   |     -      |     -     |   4.48   \n",
            "   7    |   180   |   0.160689   |     -      |     -     |   4.48   \n",
            "   7    |   200   |   0.194709   |     -      |     -     |   4.48   \n",
            "   7    |   215   |   0.192857   |     -      |     -     |   3.32   \n",
            "----------------------------------------------------------------------\n",
            "   7    |    -    |   0.194517   |  0.235869  |   91.28   |   49.79  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  | Val. Acuracia |   Tempo  \n",
            "----------------------------------------------------------------------\n",
            "   8    |   20    |   0.216166   |     -      |     -     |   4.69   \n",
            "   8    |   40    |   0.181779   |     -      |     -     |   4.48   \n",
            "   8    |   60    |   0.210134   |     -      |     -     |   4.48   \n",
            "   8    |   80    |   0.145686   |     -      |     -     |   4.47   \n",
            "   8    |   100   |   0.180471   |     -      |     -     |   4.49   \n",
            "   8    |   120   |   0.162479   |     -      |     -     |   4.47   \n",
            "   8    |   140   |   0.203548   |     -      |     -     |   4.48   \n",
            "   8    |   160   |   0.180353   |     -      |     -     |   4.48   \n",
            "   8    |   180   |   0.203613   |     -      |     -     |   4.48   \n",
            "   8    |   200   |   0.234392   |     -      |     -     |   4.48   \n",
            "   8    |   215   |   0.218808   |     -      |     -     |   3.32   \n",
            "----------------------------------------------------------------------\n",
            "   8    |    -    |   0.193846   |  0.235869  |   91.28   |   49.75  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  | Val. Acuracia |   Tempo  \n",
            "----------------------------------------------------------------------\n",
            "   9    |   20    |   0.168134   |     -      |     -     |   4.68   \n",
            "   9    |   40    |   0.182824   |     -      |     -     |   4.48   \n",
            "   9    |   60    |   0.218962   |     -      |     -     |   4.48   \n",
            "   9    |   80    |   0.178271   |     -      |     -     |   4.49   \n",
            "   9    |   100   |   0.214137   |     -      |     -     |   4.48   \n",
            "   9    |   120   |   0.210743   |     -      |     -     |   4.48   \n",
            "   9    |   140   |   0.181328   |     -      |     -     |   4.47   \n",
            "   9    |   160   |   0.200966   |     -      |     -     |   4.49   \n",
            "   9    |   180   |   0.201010   |     -      |     -     |   4.48   \n",
            "   9    |   200   |   0.193302   |     -      |     -     |   4.48   \n",
            "   9    |   215   |   0.166588   |     -      |     -     |   3.32   \n",
            "----------------------------------------------------------------------\n",
            "   9    |    -    |   0.192873   |  0.235869  |   91.28   |   49.76  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  | Val. Acuracia |   Tempo  \n",
            "----------------------------------------------------------------------\n",
            "  10    |   20    |   0.185134   |     -      |     -     |   4.69   \n",
            "  10    |   40    |   0.175568   |     -      |     -     |   4.47   \n",
            "  10    |   60    |   0.203355   |     -      |     -     |   4.48   \n",
            "  10    |   80    |   0.190045   |     -      |     -     |   4.48   \n",
            "  10    |   100   |   0.210234   |     -      |     -     |   4.47   \n",
            "  10    |   120   |   0.210131   |     -      |     -     |   4.48   \n",
            "  10    |   140   |   0.171223   |     -      |     -     |   4.48   \n",
            "  10    |   160   |   0.208880   |     -      |     -     |   4.47   \n",
            "  10    |   180   |   0.210323   |     -      |     -     |   4.47   \n",
            "  10    |   200   |   0.182504   |     -      |     -     |   4.47   \n",
            "  10    |   215   |   0.197624   |     -      |     -     |   3.32   \n",
            "----------------------------------------------------------------------\n",
            "  10    |    -    |   0.194895   |  0.235869  |   91.28   |   49.70  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  | Val. Acuracia |   Tempo  \n",
            "----------------------------------------------------------------------\n",
            "  11    |   20    |   0.185972   |     -      |     -     |   4.68   \n",
            "  11    |   40    |   0.199121   |     -      |     -     |   4.46   \n",
            "  11    |   60    |   0.210637   |     -      |     -     |   4.48   \n",
            "  11    |   80    |   0.182899   |     -      |     -     |   4.46   \n",
            "  11    |   100   |   0.202277   |     -      |     -     |   4.47   \n",
            "  11    |   120   |   0.203817   |     -      |     -     |   4.48   \n",
            "  11    |   140   |   0.244131   |     -      |     -     |   4.48   \n",
            "  11    |   160   |   0.178345   |     -      |     -     |   4.48   \n",
            "  11    |   180   |   0.183825   |     -      |     -     |   4.48   \n",
            "  11    |   200   |   0.164754   |     -      |     -     |   4.48   \n",
            "  11    |   215   |   0.158651   |     -      |     -     |   3.31   \n",
            "----------------------------------------------------------------------\n",
            "  11    |    -    |   0.192969   |  0.235869  |   91.28   |   49.70  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  | Val. Acuracia |   Tempo  \n",
            "----------------------------------------------------------------------\n",
            "  12    |   20    |   0.200457   |     -      |     -     |   4.69   \n",
            "  12    |   40    |   0.177567   |     -      |     -     |   4.48   \n",
            "  12    |   60    |   0.223562   |     -      |     -     |   4.48   \n",
            "  12    |   80    |   0.198835   |     -      |     -     |   4.48   \n",
            "  12    |   100   |   0.184341   |     -      |     -     |   4.48   \n",
            "  12    |   120   |   0.218544   |     -      |     -     |   4.49   \n",
            "  12    |   140   |   0.148688   |     -      |     -     |   4.48   \n",
            "  12    |   160   |   0.190592   |     -      |     -     |   4.49   \n",
            "  12    |   180   |   0.195566   |     -      |     -     |   4.48   \n",
            "  12    |   200   |   0.181550   |     -      |     -     |   4.48   \n",
            "  12    |   215   |   0.204745   |     -      |     -     |   3.32   \n",
            "----------------------------------------------------------------------\n",
            "  12    |    -    |   0.192897   |  0.235869  |   91.28   |   49.79  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  | Val. Acuracia |   Tempo  \n",
            "----------------------------------------------------------------------\n",
            "  13    |   20    |   0.197149   |     -      |     -     |   4.69   \n",
            "  13    |   40    |   0.192474   |     -      |     -     |   4.49   \n",
            "  13    |   60    |   0.166183   |     -      |     -     |   4.49   \n",
            "  13    |   80    |   0.176812   |     -      |     -     |   4.49   \n",
            "  13    |   100   |   0.214340   |     -      |     -     |   4.48   \n",
            "  13    |   120   |   0.189987   |     -      |     -     |   4.48   \n",
            "  13    |   140   |   0.170901   |     -      |     -     |   4.47   \n",
            "  13    |   160   |   0.225229   |     -      |     -     |   4.49   \n",
            "  13    |   180   |   0.192320   |     -      |     -     |   4.49   \n",
            "  13    |   200   |   0.170693   |     -      |     -     |   4.48   \n",
            "  13    |   215   |   0.224985   |     -      |     -     |   3.32   \n",
            "----------------------------------------------------------------------\n",
            "  13    |    -    |   0.192100   |  0.235869  |   91.28   |   49.79  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  | Val. Acuracia |   Tempo  \n",
            "----------------------------------------------------------------------\n",
            "  14    |   20    |   0.234495   |     -      |     -     |   4.69   \n",
            "  14    |   40    |   0.200179   |     -      |     -     |   4.49   \n",
            "  14    |   60    |   0.212387   |     -      |     -     |   4.48   \n",
            "  14    |   80    |   0.192846   |     -      |     -     |   4.48   \n",
            "  14    |   100   |   0.187608   |     -      |     -     |   4.49   \n",
            "  14    |   120   |   0.149207   |     -      |     -     |   4.48   \n",
            "  14    |   140   |   0.204295   |     -      |     -     |   4.49   \n",
            "  14    |   160   |   0.175662   |     -      |     -     |   4.49   \n",
            "  14    |   180   |   0.159539   |     -      |     -     |   4.49   \n",
            "  14    |   200   |   0.214472   |     -      |     -     |   4.48   \n",
            "  14    |   215   |   0.173333   |     -      |     -     |   3.33   \n",
            "----------------------------------------------------------------------\n",
            "  14    |    -    |   0.191890   |  0.235869  |   91.28   |   49.82  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  | Val. Acuracia |   Tempo  \n",
            "----------------------------------------------------------------------\n",
            "  15    |   20    |   0.200319   |     -      |     -     |   4.70   \n",
            "  15    |   40    |   0.175062   |     -      |     -     |   4.48   \n",
            "  15    |   60    |   0.198943   |     -      |     -     |   4.47   \n",
            "  15    |   80    |   0.229177   |     -      |     -     |   4.48   \n",
            "  15    |   100   |   0.190460   |     -      |     -     |   4.49   \n",
            "  15    |   120   |   0.173283   |     -      |     -     |   4.48   \n",
            "  15    |   140   |   0.182328   |     -      |     -     |   4.49   \n",
            "  15    |   160   |   0.206816   |     -      |     -     |   4.48   \n",
            "  15    |   180   |   0.189262   |     -      |     -     |   4.48   \n",
            "  15    |   200   |   0.172877   |     -      |     -     |   4.48   \n",
            "  15    |   215   |   0.202194   |     -      |     -     |   3.33   \n",
            "----------------------------------------------------------------------\n",
            "  15    |    -    |   0.192610   |  0.235869  |   91.28   |   49.80  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  | Val. Acuracia |   Tempo  \n",
            "----------------------------------------------------------------------\n",
            "  16    |   20    |   0.205613   |     -      |     -     |   4.69   \n",
            "  16    |   40    |   0.175594   |     -      |     -     |   4.48   \n",
            "  16    |   60    |   0.191562   |     -      |     -     |   4.48   \n",
            "  16    |   80    |   0.177717   |     -      |     -     |   4.49   \n",
            "  16    |   100   |   0.218322   |     -      |     -     |   4.49   \n",
            "  16    |   120   |   0.185658   |     -      |     -     |   4.48   \n",
            "  16    |   140   |   0.193067   |     -      |     -     |   4.49   \n",
            "  16    |   160   |   0.203021   |     -      |     -     |   4.47   \n",
            "  16    |   180   |   0.195251   |     -      |     -     |   4.48   \n",
            "  16    |   200   |   0.182050   |     -      |     -     |   4.48   \n",
            "  16    |   215   |   0.209939   |     -      |     -     |   3.33   \n",
            "----------------------------------------------------------------------\n",
            "  16    |    -    |   0.194036   |  0.235869  |   91.28   |   49.79  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  | Val. Acuracia |   Tempo  \n",
            "----------------------------------------------------------------------\n",
            "  17    |   20    |   0.151623   |     -      |     -     |   4.70   \n",
            "  17    |   40    |   0.191584   |     -      |     -     |   4.49   \n",
            "  17    |   60    |   0.200016   |     -      |     -     |   4.50   \n",
            "  17    |   80    |   0.173485   |     -      |     -     |   4.49   \n",
            "  17    |   100   |   0.221849   |     -      |     -     |   4.49   \n",
            "  17    |   120   |   0.183037   |     -      |     -     |   4.49   \n",
            "  17    |   140   |   0.169979   |     -      |     -     |   4.49   \n",
            "  17    |   160   |   0.221995   |     -      |     -     |   4.49   \n",
            "  17    |   180   |   0.226118   |     -      |     -     |   4.48   \n",
            "  17    |   200   |   0.206481   |     -      |     -     |   4.49   \n",
            "  17    |   215   |   0.174947   |     -      |     -     |   3.33   \n",
            "----------------------------------------------------------------------\n",
            "  17    |    -    |   0.193052   |  0.235869  |   91.28   |   49.87  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  | Val. Acuracia |   Tempo  \n",
            "----------------------------------------------------------------------\n",
            "  18    |   20    |   0.180178   |     -      |     -     |   4.70   \n",
            "  18    |   40    |   0.208093   |     -      |     -     |   4.49   \n",
            "  18    |   60    |   0.221570   |     -      |     -     |   4.48   \n",
            "  18    |   80    |   0.198727   |     -      |     -     |   4.47   \n",
            "  18    |   100   |   0.184700   |     -      |     -     |   4.48   \n",
            "  18    |   120   |   0.168878   |     -      |     -     |   4.49   \n",
            "  18    |   140   |   0.180212   |     -      |     -     |   4.47   \n",
            "  18    |   160   |   0.189165   |     -      |     -     |   4.48   \n",
            "  18    |   180   |   0.213651   |     -      |     -     |   4.49   \n",
            "  18    |   200   |   0.199798   |     -      |     -     |   4.49   \n",
            "  18    |   215   |   0.187267   |     -      |     -     |   3.32   \n",
            "----------------------------------------------------------------------\n",
            "  18    |    -    |   0.193929   |  0.235869  |   91.28   |   49.78  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  | Val. Acuracia |   Tempo  \n",
            "----------------------------------------------------------------------\n",
            "  19    |   20    |   0.206093   |     -      |     -     |   4.70   \n",
            "  19    |   40    |   0.206659   |     -      |     -     |   4.48   \n",
            "  19    |   60    |   0.192760   |     -      |     -     |   4.48   \n",
            "  19    |   80    |   0.218192   |     -      |     -     |   4.47   \n",
            "  19    |   100   |   0.208710   |     -      |     -     |   4.48   \n",
            "  19    |   120   |   0.149992   |     -      |     -     |   4.48   \n",
            "  19    |   140   |   0.171867   |     -      |     -     |   4.49   \n",
            "  19    |   160   |   0.190126   |     -      |     -     |   4.48   \n",
            "  19    |   180   |   0.186209   |     -      |     -     |   4.49   \n",
            "  19    |   200   |   0.174691   |     -      |     -     |   4.47   \n",
            "  19    |   215   |   0.209911   |     -      |     -     |   3.33   \n",
            "----------------------------------------------------------------------\n",
            "  19    |    -    |   0.191948   |  0.235869  |   91.28   |   49.78  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  | Val. Acuracia |   Tempo  \n",
            "----------------------------------------------------------------------\n",
            "  20    |   20    |   0.215898   |     -      |     -     |   4.69   \n",
            "  20    |   40    |   0.175890   |     -      |     -     |   4.48   \n",
            "  20    |   60    |   0.177569   |     -      |     -     |   4.47   \n",
            "  20    |   80    |   0.174192   |     -      |     -     |   4.47   \n",
            "  20    |   100   |   0.192382   |     -      |     -     |   4.48   \n",
            "  20    |   120   |   0.208083   |     -      |     -     |   4.47   \n",
            "  20    |   140   |   0.189248   |     -      |     -     |   4.48   \n",
            "  20    |   160   |   0.188736   |     -      |     -     |   4.47   \n",
            "  20    |   180   |   0.213707   |     -      |     -     |   4.46   \n",
            "  20    |   200   |   0.170708   |     -      |     -     |   4.46   \n",
            "  20    |   215   |   0.236484   |     -      |     -     |   3.32   \n",
            "----------------------------------------------------------------------\n",
            "  20    |    -    |   0.193942   |  0.235869  |   91.28   |   49.69  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  | Val. Acuracia |   Tempo  \n",
            "----------------------------------------------------------------------\n",
            "  21    |   20    |   0.183976   |     -      |     -     |   4.69   \n",
            "  21    |   40    |   0.175837   |     -      |     -     |   4.48   \n",
            "  21    |   60    |   0.185493   |     -      |     -     |   4.47   \n",
            "  21    |   80    |   0.188985   |     -      |     -     |   4.48   \n",
            "  21    |   100   |   0.201243   |     -      |     -     |   4.47   \n",
            "  21    |   120   |   0.205599   |     -      |     -     |   4.48   \n",
            "  21    |   140   |   0.174466   |     -      |     -     |   4.48   \n",
            "  21    |   160   |   0.222761   |     -      |     -     |   4.48   \n",
            "  21    |   180   |   0.193317   |     -      |     -     |   4.47   \n",
            "  21    |   200   |   0.212357   |     -      |     -     |   4.47   \n",
            "  21    |   215   |   0.168205   |     -      |     -     |   3.32   \n",
            "----------------------------------------------------------------------\n",
            "  21    |    -    |   0.192536   |  0.235869  |   91.28   |   49.71  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  | Val. Acuracia |   Tempo  \n",
            "----------------------------------------------------------------------\n",
            "  22    |   20    |   0.210575   |     -      |     -     |   4.69   \n",
            "  22    |   40    |   0.160169   |     -      |     -     |   4.47   \n",
            "  22    |   60    |   0.199392   |     -      |     -     |   4.48   \n",
            "  22    |   80    |   0.186881   |     -      |     -     |   4.47   \n",
            "  22    |   100   |   0.211407   |     -      |     -     |   4.47   \n",
            "  22    |   120   |   0.176012   |     -      |     -     |   4.48   \n",
            "  22    |   140   |   0.180370   |     -      |     -     |   4.48   \n",
            "  22    |   160   |   0.232657   |     -      |     -     |   4.47   \n",
            "  22    |   180   |   0.173690   |     -      |     -     |   4.47   \n",
            "  22    |   200   |   0.208759   |     -      |     -     |   4.48   \n",
            "  22    |   215   |   0.181284   |     -      |     -     |   3.30   \n",
            "----------------------------------------------------------------------\n",
            "  22    |    -    |   0.193186   |  0.235869  |   91.28   |   49.70  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  | Val. Acuracia |   Tempo  \n",
            "----------------------------------------------------------------------\n",
            "  23    |   20    |   0.201287   |     -      |     -     |   4.69   \n",
            "  23    |   40    |   0.243014   |     -      |     -     |   4.48   \n",
            "  23    |   60    |   0.164090   |     -      |     -     |   4.47   \n",
            "  23    |   80    |   0.174639   |     -      |     -     |   4.47   \n",
            "  23    |   100   |   0.218701   |     -      |     -     |   4.47   \n",
            "  23    |   120   |   0.201915   |     -      |     -     |   4.48   \n",
            "  23    |   140   |   0.152533   |     -      |     -     |   4.47   \n",
            "  23    |   160   |   0.189445   |     -      |     -     |   4.47   \n",
            "  23    |   180   |   0.189980   |     -      |     -     |   4.47   \n",
            "  23    |   200   |   0.180976   |     -      |     -     |   4.47   \n",
            "  23    |   215   |   0.195335   |     -      |     -     |   3.32   \n",
            "----------------------------------------------------------------------\n",
            "  23    |    -    |   0.191958   |  0.235869  |   91.28   |   49.66  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  | Val. Acuracia |   Tempo  \n",
            "----------------------------------------------------------------------\n",
            "  24    |   20    |   0.217160   |     -      |     -     |   4.69   \n",
            "  24    |   40    |   0.170581   |     -      |     -     |   4.47   \n",
            "  24    |   60    |   0.176348   |     -      |     -     |   4.49   \n",
            "  24    |   80    |   0.174592   |     -      |     -     |   4.48   \n",
            "  24    |   100   |   0.195900   |     -      |     -     |   4.49   \n",
            "  24    |   120   |   0.200340   |     -      |     -     |   4.48   \n",
            "  24    |   140   |   0.196802   |     -      |     -     |   4.47   \n",
            "  24    |   160   |   0.167293   |     -      |     -     |   4.47   \n",
            "  24    |   180   |   0.209362   |     -      |     -     |   4.47   \n",
            "  24    |   200   |   0.193619   |     -      |     -     |   4.48   \n",
            "  24    |   215   |   0.218659   |     -      |     -     |   3.32   \n",
            "----------------------------------------------------------------------\n",
            "  24    |    -    |   0.192301   |  0.235869  |   91.28   |   49.73  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  | Val. Acuracia |   Tempo  \n",
            "----------------------------------------------------------------------\n",
            "  25    |   20    |   0.197243   |     -      |     -     |   4.69   \n",
            "  25    |   40    |   0.186974   |     -      |     -     |   4.48   \n",
            "  25    |   60    |   0.215122   |     -      |     -     |   4.47   \n",
            "  25    |   80    |   0.156944   |     -      |     -     |   4.47   \n",
            "  25    |   100   |   0.190313   |     -      |     -     |   4.48   \n",
            "  25    |   120   |   0.206532   |     -      |     -     |   4.47   \n",
            "  25    |   140   |   0.181185   |     -      |     -     |   4.47   \n",
            "  25    |   160   |   0.233143   |     -      |     -     |   4.48   \n",
            "  25    |   180   |   0.191125   |     -      |     -     |   4.49   \n",
            "  25    |   200   |   0.175642   |     -      |     -     |   4.47   \n",
            "  25    |   215   |   0.174093   |     -      |     -     |   3.31   \n",
            "----------------------------------------------------------------------\n",
            "  25    |    -    |   0.192098   |  0.235869  |   91.28   |   49.70  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  | Val. Acuracia |   Tempo  \n",
            "----------------------------------------------------------------------\n",
            "  26    |   20    |   0.204810   |     -      |     -     |   4.69   \n",
            "  26    |   40    |   0.161830   |     -      |     -     |   4.47   \n",
            "  26    |   60    |   0.175571   |     -      |     -     |   4.46   \n",
            "  26    |   80    |   0.202628   |     -      |     -     |   4.47   \n",
            "  26    |   100   |   0.203763   |     -      |     -     |   4.48   \n",
            "  26    |   120   |   0.194362   |     -      |     -     |   4.47   \n",
            "  26    |   140   |   0.195435   |     -      |     -     |   4.47   \n",
            "  26    |   160   |   0.191461   |     -      |     -     |   4.46   \n",
            "  26    |   180   |   0.188995   |     -      |     -     |   4.48   \n",
            "  26    |   200   |   0.180641   |     -      |     -     |   4.47   \n",
            "  26    |   215   |   0.214993   |     -      |     -     |   3.32   \n",
            "----------------------------------------------------------------------\n",
            "  26    |    -    |   0.191758   |  0.235869  |   91.28   |   49.65  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  | Val. Acuracia |   Tempo  \n",
            "----------------------------------------------------------------------\n",
            "  27    |   20    |   0.222649   |     -      |     -     |   4.68   \n",
            "  27    |   40    |   0.196598   |     -      |     -     |   4.46   \n",
            "  27    |   60    |   0.171765   |     -      |     -     |   4.48   \n",
            "  27    |   80    |   0.168606   |     -      |     -     |   4.47   \n",
            "  27    |   100   |   0.179547   |     -      |     -     |   4.45   \n",
            "  27    |   120   |   0.191938   |     -      |     -     |   4.48   \n",
            "  27    |   140   |   0.181157   |     -      |     -     |   4.46   \n",
            "  27    |   160   |   0.229080   |     -      |     -     |   4.47   \n",
            "  27    |   180   |   0.167149   |     -      |     -     |   4.47   \n",
            "  27    |   200   |   0.219490   |     -      |     -     |   4.47   \n",
            "  27    |   215   |   0.204812   |     -      |     -     |   3.32   \n",
            "----------------------------------------------------------------------\n",
            "  27    |    -    |   0.193770   |  0.235869  |   91.28   |   49.64  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  | Val. Acuracia |   Tempo  \n",
            "----------------------------------------------------------------------\n",
            "  28    |   20    |   0.187454   |     -      |     -     |   4.68   \n",
            "  28    |   40    |   0.216980   |     -      |     -     |   4.47   \n",
            "  28    |   60    |   0.211657   |     -      |     -     |   4.48   \n",
            "  28    |   80    |   0.137138   |     -      |     -     |   4.47   \n",
            "  28    |   100   |   0.177523   |     -      |     -     |   4.46   \n",
            "  28    |   120   |   0.207121   |     -      |     -     |   4.47   \n",
            "  28    |   140   |   0.192861   |     -      |     -     |   4.46   \n",
            "  28    |   160   |   0.191452   |     -      |     -     |   4.47   \n",
            "  28    |   180   |   0.201177   |     -      |     -     |   4.47   \n",
            "  28    |   200   |   0.177374   |     -      |     -     |   4.46   \n",
            "  28    |   215   |   0.230942   |     -      |     -     |   3.31   \n",
            "----------------------------------------------------------------------\n",
            "  28    |    -    |   0.192900   |  0.235869  |   91.28   |   49.62  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  | Val. Acuracia |   Tempo  \n",
            "----------------------------------------------------------------------\n",
            "  29    |   20    |   0.186345   |     -      |     -     |   4.68   \n",
            "  29    |   40    |   0.194281   |     -      |     -     |   4.47   \n",
            "  29    |   60    |   0.221725   |     -      |     -     |   4.47   \n",
            "  29    |   80    |   0.197421   |     -      |     -     |   4.47   \n",
            "  29    |   100   |   0.244432   |     -      |     -     |   4.47   \n",
            "  29    |   120   |   0.191679   |     -      |     -     |   4.47   \n",
            "  29    |   140   |   0.197528   |     -      |     -     |   4.47   \n",
            "  29    |   160   |   0.160576   |     -      |     -     |   4.48   \n",
            "  29    |   180   |   0.195176   |     -      |     -     |   4.46   \n",
            "  29    |   200   |   0.166680   |     -      |     -     |   4.47   \n",
            "  29    |   215   |   0.155541   |     -      |     -     |   3.30   \n",
            "----------------------------------------------------------------------\n",
            "  29    |    -    |   0.192761   |  0.235869  |   91.28   |   49.65  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  | Val. Acuracia |   Tempo  \n",
            "----------------------------------------------------------------------\n",
            "  30    |   20    |   0.146134   |     -      |     -     |   4.68   \n",
            "  30    |   40    |   0.220457   |     -      |     -     |   4.46   \n",
            "  30    |   60    |   0.207710   |     -      |     -     |   4.48   \n",
            "  30    |   80    |   0.212047   |     -      |     -     |   4.46   \n",
            "  30    |   100   |   0.158976   |     -      |     -     |   4.46   \n",
            "  30    |   120   |   0.193413   |     -      |     -     |   4.46   \n",
            "  30    |   140   |   0.171598   |     -      |     -     |   4.47   \n",
            "  30    |   160   |   0.175627   |     -      |     -     |   4.46   \n",
            "  30    |   180   |   0.226146   |     -      |     -     |   4.47   \n",
            "  30    |   200   |   0.207668   |     -      |     -     |   4.47   \n",
            "  30    |   215   |   0.202763   |     -      |     -     |   3.31   \n",
            "----------------------------------------------------------------------\n",
            "  30    |    -    |   0.192514   |  0.235869  |   91.28   |   49.61  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  | Val. Acuracia |   Tempo  \n",
            "----------------------------------------------------------------------\n",
            "  31    |   20    |   0.205664   |     -      |     -     |   4.68   \n",
            "  31    |   40    |   0.158891   |     -      |     -     |   4.48   \n",
            "  31    |   60    |   0.199124   |     -      |     -     |   4.46   \n",
            "  31    |   80    |   0.200526   |     -      |     -     |   4.48   \n",
            "  31    |   100   |   0.162475   |     -      |     -     |   4.48   \n",
            "  31    |   120   |   0.188039   |     -      |     -     |   4.47   \n",
            "  31    |   140   |   0.185013   |     -      |     -     |   4.48   \n",
            "  31    |   160   |   0.213834   |     -      |     -     |   4.47   \n",
            "  31    |   180   |   0.176875   |     -      |     -     |   4.47   \n",
            "  31    |   200   |   0.205583   |     -      |     -     |   4.48   \n",
            "  31    |   215   |   0.252621   |     -      |     -     |   3.32   \n",
            "----------------------------------------------------------------------\n",
            "  31    |    -    |   0.194053   |  0.235869  |   91.28   |   49.69  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  | Val. Acuracia |   Tempo  \n",
            "----------------------------------------------------------------------\n",
            "  32    |   20    |   0.169980   |     -      |     -     |   4.69   \n",
            "  32    |   40    |   0.206102   |     -      |     -     |   4.48   \n",
            "  32    |   60    |   0.194020   |     -      |     -     |   4.47   \n",
            "  32    |   80    |   0.191027   |     -      |     -     |   4.48   \n",
            "  32    |   100   |   0.212890   |     -      |     -     |   4.49   \n",
            "  32    |   120   |   0.193833   |     -      |     -     |   4.48   \n",
            "  32    |   140   |   0.190549   |     -      |     -     |   4.49   \n",
            "  32    |   160   |   0.190781   |     -      |     -     |   4.47   \n",
            "  32    |   180   |   0.199886   |     -      |     -     |   4.48   \n",
            "  32    |   200   |   0.164455   |     -      |     -     |   4.47   \n",
            "  32    |   215   |   0.222489   |     -      |     -     |   3.32   \n",
            "----------------------------------------------------------------------\n",
            "  32    |    -    |   0.193416   |  0.235869  |   91.28   |   49.75  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  | Val. Acuracia |   Tempo  \n",
            "----------------------------------------------------------------------\n",
            "  33    |   20    |   0.194306   |     -      |     -     |   4.69   \n",
            "  33    |   40    |   0.203849   |     -      |     -     |   4.47   \n",
            "  33    |   60    |   0.204367   |     -      |     -     |   4.48   \n",
            "  33    |   80    |   0.224747   |     -      |     -     |   4.48   \n",
            "  33    |   100   |   0.219486   |     -      |     -     |   4.49   \n",
            "  33    |   120   |   0.182415   |     -      |     -     |   4.47   \n",
            "  33    |   140   |   0.191637   |     -      |     -     |   4.47   \n",
            "  33    |   160   |   0.175011   |     -      |     -     |   4.47   \n",
            "  33    |   180   |   0.167067   |     -      |     -     |   4.48   \n",
            "  33    |   200   |   0.206222   |     -      |     -     |   4.48   \n",
            "  33    |   215   |   0.140702   |     -      |     -     |   3.32   \n",
            "----------------------------------------------------------------------\n",
            "  33    |    -    |   0.192995   |  0.235869  |   91.28   |   49.73  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  | Val. Acuracia |   Tempo  \n",
            "----------------------------------------------------------------------\n",
            "  34    |   20    |   0.185731   |     -      |     -     |   4.69   \n",
            "  34    |   40    |   0.165817   |     -      |     -     |   4.48   \n",
            "  34    |   60    |   0.183898   |     -      |     -     |   4.48   \n",
            "  34    |   80    |   0.213025   |     -      |     -     |   4.48   \n",
            "  34    |   100   |   0.208494   |     -      |     -     |   4.47   \n",
            "  34    |   120   |   0.192914   |     -      |     -     |   4.47   \n",
            "  34    |   140   |   0.187702   |     -      |     -     |   4.48   \n",
            "  34    |   160   |   0.172109   |     -      |     -     |   4.47   \n",
            "  34    |   180   |   0.193903   |     -      |     -     |   4.47   \n",
            "  34    |   200   |   0.204765   |     -      |     -     |   4.47   \n",
            "  34    |   215   |   0.228171   |     -      |     -     |   3.32   \n",
            "----------------------------------------------------------------------\n",
            "  34    |    -    |   0.193405   |  0.235869  |   91.28   |   49.72  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  | Val. Acuracia |   Tempo  \n",
            "----------------------------------------------------------------------\n",
            "  35    |   20    |   0.176205   |     -      |     -     |   4.69   \n",
            "  35    |   40    |   0.179053   |     -      |     -     |   4.48   \n",
            "  35    |   60    |   0.195657   |     -      |     -     |   4.46   \n",
            "  35    |   80    |   0.172019   |     -      |     -     |   4.46   \n",
            "  35    |   100   |   0.187657   |     -      |     -     |   4.48   \n",
            "  35    |   120   |   0.234618   |     -      |     -     |   4.48   \n",
            "  35    |   140   |   0.208139   |     -      |     -     |   4.47   \n",
            "  35    |   160   |   0.197308   |     -      |     -     |   4.48   \n",
            "  35    |   180   |   0.185817   |     -      |     -     |   4.48   \n",
            "  35    |   200   |   0.178371   |     -      |     -     |   4.48   \n",
            "  35    |   215   |   0.219843   |     -      |     -     |   3.32   \n",
            "----------------------------------------------------------------------\n",
            "  35    |    -    |   0.193383   |  0.235869  |   91.28   |   49.72  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  | Val. Acuracia |   Tempo  \n",
            "----------------------------------------------------------------------\n",
            "  36    |   20    |   0.200560   |     -      |     -     |   4.69   \n",
            "  36    |   40    |   0.179923   |     -      |     -     |   4.48   \n",
            "  36    |   60    |   0.174024   |     -      |     -     |   4.47   \n",
            "  36    |   80    |   0.201309   |     -      |     -     |   4.48   \n",
            "  36    |   100   |   0.199969   |     -      |     -     |   4.48   \n",
            "  36    |   120   |   0.206784   |     -      |     -     |   4.48   \n",
            "  36    |   140   |   0.209198   |     -      |     -     |   4.48   \n",
            "  36    |   160   |   0.154972   |     -      |     -     |   4.48   \n",
            "  36    |   180   |   0.184922   |     -      |     -     |   4.47   \n",
            "  36    |   200   |   0.213056   |     -      |     -     |   4.47   \n",
            "  36    |   215   |   0.209978   |     -      |     -     |   3.32   \n",
            "----------------------------------------------------------------------\n",
            "  36    |    -    |   0.193725   |  0.235869  |   91.28   |   49.72  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  | Val. Acuracia |   Tempo  \n",
            "----------------------------------------------------------------------\n",
            "  37    |   20    |   0.205225   |     -      |     -     |   4.69   \n",
            "  37    |   40    |   0.183226   |     -      |     -     |   4.48   \n",
            "  37    |   60    |   0.200490   |     -      |     -     |   4.47   \n",
            "  37    |   80    |   0.158865   |     -      |     -     |   4.48   \n",
            "  37    |   100   |   0.200855   |     -      |     -     |   4.49   \n",
            "  37    |   120   |   0.179746   |     -      |     -     |   4.47   \n",
            "  37    |   140   |   0.167182   |     -      |     -     |   4.48   \n",
            "  37    |   160   |   0.151806   |     -      |     -     |   4.48   \n",
            "  37    |   180   |   0.262489   |     -      |     -     |   4.48   \n",
            "  37    |   200   |   0.207791   |     -      |     -     |   4.47   \n",
            "  37    |   215   |   0.195515   |     -      |     -     |   3.32   \n",
            "----------------------------------------------------------------------\n",
            "  37    |    -    |   0.192090   |  0.235869  |   91.28   |   49.74  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  | Val. Acuracia |   Tempo  \n",
            "----------------------------------------------------------------------\n",
            "  38    |   20    |   0.205210   |     -      |     -     |   4.68   \n",
            "  38    |   40    |   0.180402   |     -      |     -     |   4.48   \n",
            "  38    |   60    |   0.170873   |     -      |     -     |   4.48   \n",
            "  38    |   80    |   0.210742   |     -      |     -     |   4.48   \n",
            "  38    |   100   |   0.210244   |     -      |     -     |   4.48   \n",
            "  38    |   120   |   0.168065   |     -      |     -     |   4.48   \n",
            "  38    |   140   |   0.154278   |     -      |     -     |   4.47   \n",
            "  38    |   160   |   0.200345   |     -      |     -     |   4.47   \n",
            "  38    |   180   |   0.204801   |     -      |     -     |   4.47   \n",
            "  38    |   200   |   0.224794   |     -      |     -     |   4.47   \n",
            "  38    |   215   |   0.202720   |     -      |     -     |   3.32   \n",
            "----------------------------------------------------------------------\n",
            "  38    |    -    |   0.193709   |  0.235869  |   91.28   |   49.72  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  | Val. Acuracia |   Tempo  \n",
            "----------------------------------------------------------------------\n",
            "  39    |   20    |   0.215041   |     -      |     -     |   4.68   \n",
            "  39    |   40    |   0.213502   |     -      |     -     |   4.47   \n",
            "  39    |   60    |   0.228222   |     -      |     -     |   4.48   \n",
            "  39    |   80    |   0.198825   |     -      |     -     |   4.47   \n",
            "  39    |   100   |   0.185080   |     -      |     -     |   4.47   \n",
            "  39    |   120   |   0.209462   |     -      |     -     |   4.47   \n",
            "  39    |   140   |   0.183241   |     -      |     -     |   4.47   \n",
            "  39    |   160   |   0.161906   |     -      |     -     |   4.46   \n",
            "  39    |   180   |   0.190753   |     -      |     -     |   4.47   \n",
            "  39    |   200   |   0.179111   |     -      |     -     |   4.47   \n",
            "  39    |   215   |   0.144008   |     -      |     -     |   3.32   \n",
            "----------------------------------------------------------------------\n",
            "  39    |    -    |   0.192954   |  0.235869  |   91.28   |   49.67  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  | Val. Acuracia |   Tempo  \n",
            "----------------------------------------------------------------------\n",
            "  40    |   20    |   0.185420   |     -      |     -     |   4.68   \n",
            "  40    |   40    |   0.198272   |     -      |     -     |   4.47   \n",
            "  40    |   60    |   0.197507   |     -      |     -     |   4.47   \n",
            "  40    |   80    |   0.130538   |     -      |     -     |   4.47   \n",
            "  40    |   100   |   0.222114   |     -      |     -     |   4.47   \n",
            "  40    |   120   |   0.215126   |     -      |     -     |   4.47   \n",
            "  40    |   140   |   0.209439   |     -      |     -     |   4.47   \n",
            "  40    |   160   |   0.171166   |     -      |     -     |   4.48   \n",
            "  40    |   180   |   0.215498   |     -      |     -     |   4.47   \n",
            "  40    |   200   |   0.194027   |     -      |     -     |   4.48   \n",
            "  40    |   215   |   0.182242   |     -      |     -     |   3.31   \n",
            "----------------------------------------------------------------------\n",
            "  40    |    -    |   0.193061   |  0.235869  |   91.28   |   49.67  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  | Val. Acuracia |   Tempo  \n",
            "----------------------------------------------------------------------\n",
            "  41    |   20    |   0.175672   |     -      |     -     |   4.69   \n",
            "  41    |   40    |   0.203541   |     -      |     -     |   4.47   \n",
            "  41    |   60    |   0.186408   |     -      |     -     |   4.47   \n",
            "  41    |   80    |   0.223660   |     -      |     -     |   4.46   \n",
            "  41    |   100   |   0.198257   |     -      |     -     |   4.46   \n",
            "  41    |   120   |   0.175527   |     -      |     -     |   4.46   \n",
            "  41    |   140   |   0.205855   |     -      |     -     |   4.47   \n",
            "  41    |   160   |   0.164158   |     -      |     -     |   4.47   \n",
            "  41    |   180   |   0.198647   |     -      |     -     |   4.47   \n",
            "  41    |   200   |   0.197692   |     -      |     -     |   4.47   \n",
            "  41    |   215   |   0.205976   |     -      |     -     |   3.31   \n",
            "----------------------------------------------------------------------\n",
            "  41    |    -    |   0.193767   |  0.235869  |   91.28   |   49.63  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  | Val. Acuracia |   Tempo  \n",
            "----------------------------------------------------------------------\n",
            "  42    |   20    |   0.208218   |     -      |     -     |   4.68   \n",
            "  42    |   40    |   0.198040   |     -      |     -     |   4.47   \n",
            "  42    |   60    |   0.185655   |     -      |     -     |   4.47   \n",
            "  42    |   80    |   0.185605   |     -      |     -     |   4.47   \n",
            "  42    |   100   |   0.192292   |     -      |     -     |   4.47   \n",
            "  42    |   120   |   0.182659   |     -      |     -     |   4.47   \n",
            "  42    |   140   |   0.174138   |     -      |     -     |   4.47   \n",
            "  42    |   160   |   0.189003   |     -      |     -     |   4.48   \n",
            "  42    |   180   |   0.209890   |     -      |     -     |   4.47   \n",
            "  42    |   200   |   0.199721   |     -      |     -     |   4.46   \n",
            "  42    |   215   |   0.192881   |     -      |     -     |   3.31   \n",
            "----------------------------------------------------------------------\n",
            "  42    |    -    |   0.192620   |  0.235869  |   91.28   |   49.63  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  | Val. Acuracia |   Tempo  \n",
            "----------------------------------------------------------------------\n",
            "  43    |   20    |   0.163740   |     -      |     -     |   4.67   \n",
            "  43    |   40    |   0.187739   |     -      |     -     |   4.46   \n",
            "  43    |   60    |   0.197116   |     -      |     -     |   4.47   \n",
            "  43    |   80    |   0.184291   |     -      |     -     |   4.47   \n",
            "  43    |   100   |   0.200487   |     -      |     -     |   4.46   \n",
            "  43    |   120   |   0.208346   |     -      |     -     |   4.46   \n",
            "  43    |   140   |   0.161850   |     -      |     -     |   4.46   \n",
            "  43    |   160   |   0.222929   |     -      |     -     |   4.47   \n",
            "  43    |   180   |   0.185782   |     -      |     -     |   4.47   \n",
            "  43    |   200   |   0.201134   |     -      |     -     |   4.47   \n",
            "  43    |   215   |   0.216271   |     -      |     -     |   3.31   \n",
            "----------------------------------------------------------------------\n",
            "  43    |    -    |   0.192945   |  0.235869  |   91.28   |   49.62  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  | Val. Acuracia |   Tempo  \n",
            "----------------------------------------------------------------------\n",
            "  44    |   20    |   0.200636   |     -      |     -     |   4.66   \n",
            "  44    |   40    |   0.209540   |     -      |     -     |   4.47   \n",
            "  44    |   60    |   0.211106   |     -      |     -     |   4.47   \n",
            "  44    |   80    |   0.206856   |     -      |     -     |   4.46   \n",
            "  44    |   100   |   0.188051   |     -      |     -     |   4.47   \n",
            "  44    |   120   |   0.176016   |     -      |     -     |   4.47   \n",
            "  44    |   140   |   0.211410   |     -      |     -     |   4.46   \n",
            "  44    |   160   |   0.185421   |     -      |     -     |   4.47   \n",
            "  44    |   180   |   0.187878   |     -      |     -     |   4.46   \n",
            "  44    |   200   |   0.165490   |     -      |     -     |   4.47   \n",
            "  44    |   215   |   0.164543   |     -      |     -     |   3.31   \n",
            "----------------------------------------------------------------------\n",
            "  44    |    -    |   0.192207   |  0.235869  |   91.28   |   49.59  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  | Val. Acuracia |   Tempo  \n",
            "----------------------------------------------------------------------\n",
            "  45    |   20    |   0.183518   |     -      |     -     |   4.68   \n",
            "  45    |   40    |   0.181052   |     -      |     -     |   4.47   \n",
            "  45    |   60    |   0.210873   |     -      |     -     |   4.47   \n",
            "  45    |   80    |   0.145718   |     -      |     -     |   4.47   \n",
            "  45    |   100   |   0.197365   |     -      |     -     |   4.46   \n",
            "  45    |   120   |   0.190559   |     -      |     -     |   4.48   \n",
            "  45    |   140   |   0.207576   |     -      |     -     |   4.47   \n",
            "  45    |   160   |   0.167956   |     -      |     -     |   4.47   \n",
            "  45    |   180   |   0.185194   |     -      |     -     |   4.46   \n",
            "  45    |   200   |   0.226054   |     -      |     -     |   4.47   \n",
            "  45    |   215   |   0.225807   |     -      |     -     |   3.31   \n",
            "----------------------------------------------------------------------\n",
            "  45    |    -    |   0.192074   |  0.235869  |   91.28   |   49.62  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  | Val. Acuracia |   Tempo  \n",
            "----------------------------------------------------------------------\n",
            "  46    |   20    |   0.189580   |     -      |     -     |   4.68   \n",
            "  46    |   40    |   0.212027   |     -      |     -     |   4.47   \n",
            "  46    |   60    |   0.181953   |     -      |     -     |   4.47   \n",
            "  46    |   80    |   0.216606   |     -      |     -     |   4.47   \n",
            "  46    |   100   |   0.221229   |     -      |     -     |   4.47   \n",
            "  46    |   120   |   0.159772   |     -      |     -     |   4.47   \n",
            "  46    |   140   |   0.213736   |     -      |     -     |   4.47   \n",
            "  46    |   160   |   0.181253   |     -      |     -     |   4.46   \n",
            "  46    |   180   |   0.174276   |     -      |     -     |   4.47   \n",
            "  46    |   200   |   0.176754   |     -      |     -     |   4.46   \n",
            "  46    |   215   |   0.210302   |     -      |     -     |   3.31   \n",
            "----------------------------------------------------------------------\n",
            "  46    |    -    |   0.193925   |  0.235869  |   91.28   |   49.61  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  | Val. Acuracia |   Tempo  \n",
            "----------------------------------------------------------------------\n",
            "  47    |   20    |   0.186316   |     -      |     -     |   4.67   \n",
            "  47    |   40    |   0.198150   |     -      |     -     |   4.45   \n",
            "  47    |   60    |   0.216856   |     -      |     -     |   4.47   \n",
            "  47    |   80    |   0.183200   |     -      |     -     |   4.47   \n",
            "  47    |   100   |   0.186501   |     -      |     -     |   4.46   \n",
            "  47    |   120   |   0.183845   |     -      |     -     |   4.46   \n",
            "  47    |   140   |   0.163519   |     -      |     -     |   4.46   \n",
            "  47    |   160   |   0.175700   |     -      |     -     |   4.46   \n",
            "  47    |   180   |   0.219547   |     -      |     -     |   4.46   \n",
            "  47    |   200   |   0.205050   |     -      |     -     |   4.47   \n",
            "  47    |   215   |   0.182643   |     -      |     -     |   3.32   \n",
            "----------------------------------------------------------------------\n",
            "  47    |    -    |   0.191202   |  0.235869  |   91.28   |   49.59  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  | Val. Acuracia |   Tempo  \n",
            "----------------------------------------------------------------------\n",
            "  48    |   20    |   0.175974   |     -      |     -     |   4.68   \n",
            "  48    |   40    |   0.189120   |     -      |     -     |   4.47   \n",
            "  48    |   60    |   0.208791   |     -      |     -     |   4.47   \n",
            "  48    |   80    |   0.187259   |     -      |     -     |   4.46   \n",
            "  48    |   100   |   0.180776   |     -      |     -     |   4.48   \n",
            "  48    |   120   |   0.200792   |     -      |     -     |   4.47   \n",
            "  48    |   140   |   0.205324   |     -      |     -     |   4.47   \n",
            "  48    |   160   |   0.214068   |     -      |     -     |   4.47   \n",
            "  48    |   180   |   0.203543   |     -      |     -     |   4.47   \n",
            "  48    |   200   |   0.183029   |     -      |     -     |   4.46   \n",
            "  48    |   215   |   0.173642   |     -      |     -     |   3.31   \n",
            "----------------------------------------------------------------------\n",
            "  48    |    -    |   0.193306   |  0.235869  |   91.28   |   49.63  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  | Val. Acuracia |   Tempo  \n",
            "----------------------------------------------------------------------\n",
            "  49    |   20    |   0.198366   |     -      |     -     |   4.67   \n",
            "  49    |   40    |   0.188135   |     -      |     -     |   4.46   \n",
            "  49    |   60    |   0.182195   |     -      |     -     |   4.46   \n",
            "  49    |   80    |   0.239054   |     -      |     -     |   4.47   \n",
            "  49    |   100   |   0.172862   |     -      |     -     |   4.47   \n",
            "  49    |   120   |   0.210265   |     -      |     -     |   4.46   \n",
            "  49    |   140   |   0.169499   |     -      |     -     |   4.46   \n",
            "  49    |   160   |   0.145846   |     -      |     -     |   4.46   \n",
            "  49    |   180   |   0.198287   |     -      |     -     |   4.48   \n",
            "  49    |   200   |   0.227018   |     -      |     -     |   4.48   \n",
            "  49    |   215   |   0.168914   |     -      |     -     |   3.32   \n",
            "----------------------------------------------------------------------\n",
            "  49    |    -    |   0.191494   |  0.235869  |   91.28   |   49.60  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  | Val. Acuracia |   Tempo  \n",
            "----------------------------------------------------------------------\n",
            "  50    |   20    |   0.204976   |     -      |     -     |   4.68   \n",
            "  50    |   40    |   0.190739   |     -      |     -     |   4.47   \n",
            "  50    |   60    |   0.196688   |     -      |     -     |   4.46   \n",
            "  50    |   80    |   0.200386   |     -      |     -     |   4.47   \n",
            "  50    |   100   |   0.177632   |     -      |     -     |   4.46   \n",
            "  50    |   120   |   0.212441   |     -      |     -     |   4.47   \n",
            "  50    |   140   |   0.183001   |     -      |     -     |   4.47   \n",
            "  50    |   160   |   0.182643   |     -      |     -     |   4.47   \n",
            "  50    |   180   |   0.195171   |     -      |     -     |   4.47   \n",
            "  50    |   200   |   0.206058   |     -      |     -     |   4.47   \n",
            "  50    |   215   |   0.171061   |     -      |     -     |   3.32   \n",
            "----------------------------------------------------------------------\n",
            "  50    |    -    |   0.193359   |  0.235869  |   91.28   |   49.64  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Fim da etapa de treinamento!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dwWUCOhX4Rh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "outputId": "fa3adf1f-98c3-4d4a-b748-75fadb5896f1"
      },
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def bert_predict(model, test_dataloader):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    all_logits = []\n",
        "\n",
        "\n",
        "    for batch in test_dataloader:\n",
        "        \n",
        "        b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)[:2]\n",
        "\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            logits = model(b_input_ids, b_attn_mask)\n",
        "        all_logits.append(logits)\n",
        "    \n",
        "   \n",
        "    all_logits = torch.cat(all_logits, dim=0)\n",
        "\n",
        "   \n",
        "    probs = F.softmax(all_logits, dim=1).cpu().numpy()\n",
        "\n",
        "    return probs\n",
        "\n",
        "#EVALUATION================================\n",
        "\n",
        "probs = bert_predict(bert_classifier, val_dataloader)\n",
        "evaluate_roc(probs, y_val)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AUC: 0.8928\n",
            "Acurácia: 91.28%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64qrpEebYo--",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
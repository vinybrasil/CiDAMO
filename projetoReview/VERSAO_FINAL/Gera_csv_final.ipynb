{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Web Scrapping no Trip Advisor</h2>\n",
    "\n",
    "    Na mesma pasta, deve-se ter um txt com todos os restaurantes que se quer tirar os reviews. Se o txt tiver n reviews, o script vai gerar n+1 arquivos csv, sendo o n-ésimo csv o \"CSV_FINAL.csv\", onde se tem todos os reviews equalizados e binários. Por \"binários\" entenda-se que há uma coluna que, se a nota for maior ou igual a 4, terá o valor 1 e, se a nota for estritamente menor a quatro, terá o valor 0. Por \"equalizado\" entenda-se que há 50% dos reviews com a label 1 e 50% com a label 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd \n",
    "import textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display(content, filename='output.html'):\n",
    "    \n",
    "    with open(filename, 'wb') as f:\n",
    "        f.write(content)\n",
    "        webbrowser.open(filename)\n",
    "        \n",
    "def get_soup(session, url, show=False):\n",
    "    \n",
    "    r = session.get(url)\n",
    "    if show:\n",
    "        display(r.content, 'temp.html')\n",
    "\n",
    "    if r.status_code != 200:  # not OK\n",
    "        print('[get_soup] status code:', r.status_code)\n",
    "    else:\n",
    "        \n",
    "        return BeautifulSoup(r.text, 'html.parser')\n",
    "    \n",
    "    \n",
    "def post_soup(session, url, params, show=False):\n",
    "    \n",
    "    #lê html e devolve soup\n",
    "    r = session.post(url, data=params)\n",
    "\n",
    "    if show:\n",
    "        display(r.content, 'temp.html')\n",
    "\n",
    "    if r.status_code != 200:  # not OK\n",
    "        print('[post_soup] status code:', r.status_code)\n",
    "    else:\n",
    "        return BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "\n",
    "def scrape(url, lang='ALL'):\n",
    "    \n",
    "    # create session to keep all cookies (etc.) between requests\n",
    "    session = requests.Session()\n",
    "    session.headers.update({\n",
    "        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36',\n",
    "    })\n",
    "\n",
    "    items = parse(session, url + '?filterLang=' + lang)\n",
    "\n",
    "    return items\n",
    "\n",
    "def parse(session, url):\n",
    "    \n",
    "    #Pega  o numero de reviews e gera subpaginas com os reviews\n",
    "    \n",
    "    print('[parse] url:', url)\n",
    "\n",
    "    soup = get_soup(session, url)\n",
    "\n",
    "    if not soup:\n",
    "        print('[parse] no soup:', url)\n",
    "        return\n",
    "\n",
    "    num_reviews = soup.find('span', class_='reviews_header_count').text  # get text\n",
    "    num_reviews = num_reviews[1:-1]\n",
    "    num_reviews = num_reviews.replace(',', '')\n",
    "    num_reviews = float(num_reviews)  # convert text into integer\n",
    "    print('[parse] num_reviews ALL:', num_reviews)\n",
    "\n",
    "    url_template = url.replace('.html', '-or{}.html')\n",
    "    print('[parse] url_template:', url_template)\n",
    "\n",
    "    items = []\n",
    "\n",
    "    offset = 10\n",
    "\n",
    "    while (True):\n",
    "        subpage_url = url_template.format(offset)\n",
    "\n",
    "        subpage_items = parse_reviews(session, subpage_url)\n",
    "        if not subpage_items:\n",
    "            break\n",
    "\n",
    "        items += subpage_items\n",
    "\n",
    "        if len(subpage_items) < 5:\n",
    "            break\n",
    "\n",
    "        offset += 5\n",
    "\n",
    "    return items\n",
    "\n",
    "def get_reviews_ids(soup):\n",
    "    items = soup.find_all('div', attrs={'data-reviewid': True})\n",
    "\n",
    "    if items:\n",
    "        reviews_ids = [x.attrs['data-reviewid'] for x in items][::2]\n",
    "        print('[get_reviews_ids] data-reviewid:', reviews_ids)\n",
    "        return reviews_ids\n",
    "    \n",
    "def get_more(session, reviews_ids):\n",
    "    url = 'https://www.tripadvisor.com/OverlayWidgetAjax?Mode=EXPANDED_HOTEL_REVIEWS_RESP&metaReferer=Hotel_Review'\n",
    "\n",
    "    payload = {\n",
    "        'reviews': ','.join(reviews_ids),  \n",
    "        'widgetChoice': 'EXPANDED_HOTEL_REVIEW_HSX',  \n",
    "        'haveJses': 'earlyRequireDefine,amdearly,global_error,long_lived_global,apg-Hotel_Review,apg-Hotel_Review-in,bootstrap,desktop-rooms-guests-dust-en_US,responsive-calendar-templates-dust-en_US,taevents',\n",
    "        'haveCsses': 'apg-Hotel_Review-in',\n",
    "        'Action': 'install',\n",
    "    }\n",
    "    soup = post_soup(session, url, payload)\n",
    "    return soup\n",
    "\n",
    "\n",
    "def parse_reviews(session, url):\n",
    "    '''Get all reviews from one page'''\n",
    "\n",
    "    print('[parse_reviews] url:', url)\n",
    "\n",
    "    soup = get_soup(session, url)\n",
    "\n",
    "    if not soup:\n",
    "        print('[parse_reviews] no soup:', url)\n",
    "        return\n",
    "\n",
    "    reviews_ids = get_reviews_ids(soup)\n",
    "    if not reviews_ids:\n",
    "        return\n",
    "\n",
    "    soup = get_more(session, reviews_ids)\n",
    "\n",
    "    if not soup:\n",
    "        print('[parse_reviews] no soup:', url)\n",
    "        return\n",
    "\n",
    "    items = []\n",
    "\n",
    "    for idx, review in enumerate(soup.find_all('div', class_='reviewSelector')):\n",
    "        \n",
    "        bubble_rating = review.select_one('span.ui_bubble_rating')['class']\n",
    "        bubble_rating = bubble_rating[1].split('_')[-1]\n",
    "\n",
    "        item = {\n",
    "            'review_rate': bubble_rating,\n",
    "            'review_body': review.find('p', class_='partial_entry').text,\n",
    "            #'review_date': review.find('span', class_='ratingDate')['title'],  # 'ratingDate' instead of 'relativeDate'\n",
    "        }\n",
    "\n",
    "        items.append(item)\n",
    "        print('\\n--- review ---\\n')\n",
    "        for key, val in item.items():\n",
    "            print(' ', key, ':', val)\n",
    "\n",
    "    print()\n",
    "\n",
    "    return items\n",
    "\n",
    "\n",
    "def write_in_csv(items, filename='results.csv',\n",
    "                 headers=['hotel name', 'review title', 'review body',\n",
    "                          'review date', 'contributions', 'helpful vote',\n",
    "                          'user name', 'user location', 'rating'],\n",
    "                 mode='w'):\n",
    "    print('--- CSV ---')\n",
    "\n",
    "    with io.open(filename, mode, encoding=\"utf-8\") as csvfile:\n",
    "        csv_file = csv.DictWriter(csvfile, headers)\n",
    "\n",
    "        if mode == 'w':\n",
    "            csv_file.writeheader()\n",
    "\n",
    "        csv_file.writerows(items)\n",
    "\n",
    "def read(file):\n",
    "    f = open(file, \"r\")\n",
    "    for link in f :\n",
    "        if str(link) not in start_urls:\n",
    "            start_urls.append(link)\n",
    "        else:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_COLUMN = 'review_rate'\n",
    "DB_COLUMN1 = 'review_body'\n",
    "\n",
    "start_urls = []\n",
    "read('restaurantes_get.txt')\n",
    "\n",
    "lang = 'pt-br'\n",
    "\n",
    "headers = [\n",
    "    DB_COLUMN,\n",
    "    DB_COLUMN1,  \n",
    "]\n",
    "\n",
    "for url in start_urls:\n",
    "\n",
    "    # get all reviews for 'url' and 'lang'\n",
    "    items = scrape(url, lang)\n",
    "\n",
    "    if not items:\n",
    "        print('No reviews')\n",
    "    else:\n",
    "        # write in CSV\n",
    "        filename = url.split('Reviews-')[1][:-5] + '__' + lang\n",
    "        print('filename:', filename)\n",
    "        write_in_csv(items, filename + '.csv', headers, mode='w')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ler_csv(file, PASTA):\n",
    "    print(\"Lendo \", file)\n",
    "    if file.endswith(\".csv\"):\n",
    "        DIRECTORIO = os.path.join(PASTA, file)\n",
    "        data = pd.read_csv(DIRECTORIO)\n",
    "        NEW_DF = pd.DataFrame(data)\n",
    "        return NEW_DF\n",
    "    else:\n",
    "        pass \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRIMARY_DF = pd.DataFrame().rename_axis(\"Id\", index=True)\n",
    "PASTA = \"mix_dataset\"\n",
    "i = 0\n",
    "for file in os.listdir(PASTA):\n",
    "    NEW_DF = ler_csv(file, PASTA)\n",
    "    PRIMARY_DF = PRIMARY_DF.append(NEW_DF, ignore_index=True)\n",
    "    print(PRIMARY_DF.shape)\n",
    "    i += 1\n",
    "print(\"O algoritmo tem \", i, \"restaurantes.\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DATA_BINARIZADOR_RESULTADOS(data):\n",
    "    RESULTADO_BINARIO = []\n",
    "\n",
    "    for item in data['review_rate']:\n",
    "        if item < 40:\n",
    "            RESULTADO_BINARIO.append(0)\n",
    "        elif item >= 40:\n",
    "            RESULTADO_BINARIO.append(1)\n",
    "        else:\n",
    "            print(\"Deu feijoada aqui hein\")\n",
    "\n",
    "    data['RESULTADO_BINARIO'] = RESULTADO_BINARIO\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRIMARY_DF = DATA_BINARIZADOR_RESULTADOS(PRIMARY_DF)\n",
    "PRIMARY_DF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRIMARY_DF.to_csv('CSV_FINAL.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('CSV_FINAL.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(\"Unnamed: 0\", axis=1)\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "numeros = []\n",
    "while data['RESULTADO_BINARIO'].value_counts()[1] != data['RESULTADO_BINARIO'].value_counts()[0]:\n",
    " \n",
    "    x = np.random.randint(1, len(data['RESULTADO_BINARIO']))\n",
    "    \n",
    "    if x not in numeros:\n",
    "        if data['RESULTADO_BINARIO'][x] == 1:\n",
    "            data.drop(x, axis=0, inplace=True)\n",
    "            numeros.append(x)\n",
    "            \n",
    "        else:\n",
    "            pass\n",
    "    else:\n",
    "        pass\n",
    "    data = data.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('CSV_FINAL_EQUALIZADO.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

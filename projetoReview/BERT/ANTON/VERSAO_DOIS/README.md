Continuação da implementação do BERT. Agora, o dataset é equalizado (50% de positivos e negativos) e já dá pra usar o modelo pré-treinado em PT_BR.

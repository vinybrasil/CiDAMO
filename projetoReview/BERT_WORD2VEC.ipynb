{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_WORD2VEC.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5c223babb82c419891ae62977539e5a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_865dde423edc4753ab00bdfdd637a4d6",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f1cb5884a9394f96b4dfaf5432e33e91",
              "IPY_MODEL_d88c92e58d6342a9a9e4ab715af37fda"
            ]
          }
        },
        "865dde423edc4753ab00bdfdd637a4d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f1cb5884a9394f96b4dfaf5432e33e91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_fb1aa164af2e4f2fbc397a6ed2433e80",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 2,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 2,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_56f8df7d9bd14e589a67bf544973b3ef"
          }
        },
        "d88c92e58d6342a9a9e4ab715af37fda": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1ddbf3766907471fb2b093869bdd4bc8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2/2 [1:30:07&lt;00:00, 2703.97s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c07cc6940cd948d6ac9b462f2b9006fe"
          }
        },
        "fb1aa164af2e4f2fbc397a6ed2433e80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "56f8df7d9bd14e589a67bf544973b3ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1ddbf3766907471fb2b093869bdd4bc8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c07cc6940cd948d6ac9b462f2b9006fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "72f98b0d5a8049bc8b853d68a8ecd5cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_25af0ec5032543ac871e9122d17d9109",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_268fa4763409439190e4b9a8612ef0ef",
              "IPY_MODEL_851691febd674ff2932c55a8ee2314b3"
            ]
          }
        },
        "25af0ec5032543ac871e9122d17d9109": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "268fa4763409439190e4b9a8612ef0ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_05579fce55d6481486c9b979a2fe1b87",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 2,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 2,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c31eee78b2544081a5e77ab8eb7e0cf3"
          }
        },
        "851691febd674ff2932c55a8ee2314b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_23e2d51cc8a342379ae764661472a692",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2/2 [1:02:12&lt;00:00, 1866.33s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ac9489f02f2d481f830312ff0059afe3"
          }
        },
        "05579fce55d6481486c9b979a2fe1b87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c31eee78b2544081a5e77ab8eb7e0cf3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "23e2d51cc8a342379ae764661472a692": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ac9489f02f2d481f830312ff0059afe3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLDZdudtGBFZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "bc3e4857-739a-4932-a21b-7eff2a8e6b72"
      },
      "source": [
        "#https://github.com/Shivampanwar/Bert-text-classification/blob/master/bert_language_model_with_sequence_classification.ipynb\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from tqdm import  tqdm_notebook"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2rkcA2FIops",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "directory_path = '/content/drive/My Drive/BERT'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2Sle4U2Qoeb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df = pd.read_csv(os.path.join(directory_path,'labeledTrainData.tsv'),delimiter='\\t')\n",
        "test_df = pd.read_csv(os.path.join(directory_path,'testData.tsv'),delimiter='\\t')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwpjaG4xQqQ-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5c282302-19c9-4a52-cfb1-f273e1f9541b"
      },
      "source": [
        "train_df.shape,test_df.shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((25000, 3), (25000, 2))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5GjHYYORjuK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "outputId": "82935d7b-c34d-486d-dd9f-1e7c30843079"
      },
      "source": [
        "train_df.head(n=2)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5814_8</td>\n",
              "      <td>1</td>\n",
              "      <td>With all this stuff going down at the moment w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2381_9</td>\n",
              "      <td>1</td>\n",
              "      <td>\\The Classic War of the Worlds\\\" by Timothy Hi...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       id  sentiment                                             review\n",
              "0  5814_8          1  With all this stuff going down at the moment w...\n",
              "1  2381_9          1  \\The Classic War of the Worlds\\\" by Timothy Hi..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NgvJOrx_RoGL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lm_df = pd.concat([train_df[['review']],test_df[['review']]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMyGnEsCRsAh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 274
        },
        "outputId": "912e2627-cdbb-46a3-a609-2a01efafc38f"
      },
      "source": [
        "lm_df.head #juntou os dois"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method NDFrame.head of                                                   review\n",
              "0      With all this stuff going down at the moment w...\n",
              "1      \\The Classic War of the Worlds\\\" by Timothy Hi...\n",
              "2      The film starts with a manager (Nicholas Bell)...\n",
              "3      It must be assumed that those who praised this...\n",
              "4      Superbly trashy and wondrously unpretentious 8...\n",
              "...                                                  ...\n",
              "24995  Sony Pictures Classics, I'm looking at you! So...\n",
              "24996  I always felt that Ms. Merkerson had never got...\n",
              "24997  I was so disappointed in this movie. I am very...\n",
              "24998  From the opening sequence, filled with black a...\n",
              "24999  This is a great horror film for people who don...\n",
              "\n",
              "[50000 rows x 1 columns]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqjTtdQkRvUk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lm_df.review = lm_df.review.str.lower()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGyF7NLeSDt8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "outputId": "d669b8ae-41ff-46cb-efa9-fd27e8803acb"
      },
      "source": [
        "lm_df.info()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 50000 entries, 0 to 24999\n",
            "Data columns (total 1 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   review  50000 non-null  object\n",
            "dtypes: object(1)\n",
            "memory usage: 781.2+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJ08-IlHR1Hq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tqdm.pandas()\n",
        "changed_text=lm_df.review.apply(lambda x:x+\"\\n\"+\"\\n\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTE9EI9dR5cT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "956379a5-5828-4917-8584-f6a8353418ec"
      },
      "source": [
        "\n",
        "\n",
        "open(os.path.join(directory_path,'data_lm.txt'), \"w\").write(''.join(changed_text))\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "65703846"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWrAldcnSTEq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 661
        },
        "outputId": "8f30b641-7b5b-4682-a296-288ab57a14ff"
      },
      "source": [
        "!pip install pytorch_transformers\n",
        "from pytorch_transformers import *"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch_transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/b7/d3d18008a67e0b968d1ab93ad444fc05699403fa662f634b2f2c318a508b/pytorch_transformers-1.2.0-py3-none-any.whl (176kB)\n",
            "\u001b[K     |████████████████████████████████| 184kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (1.12.39)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (1.18.2)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (1.4.0)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 36.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (4.38.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (2.21.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/50/93509f906a40bffd7d175f97fd75ea328ad9bd91f48f59c4bd084c94a25e/sacremoses-0.0.41.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 39.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: botocore<1.16.0,>=1.15.39 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_transformers) (1.15.39)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_transformers) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_transformers) (0.9.5)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_transformers) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch_transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch_transformers) (7.1.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch_transformers) (0.14.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.39->boto3->pytorch_transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.39->boto3->pytorch_transformers) (2.8.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.41-cp36-none-any.whl size=893334 sha256=ab82bd855b02a106d9ca4fa7f55fd57d0ee9f4527ece593fe13b6a861169ee21\n",
            "  Stored in directory: /root/.cache/pip/wheels/22/5a/d4/b020a81249de7dc63758a34222feaa668dbe8ebfe9170cc9b1\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, sacremoses, pytorch-transformers\n",
            "Successfully installed pytorch-transformers-1.2.0 sacremoses-0.0.41 sentencepiece-0.1.85\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tsid7WOQSdZg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "f444ad03-bf9c-491b-aac1-a9b2aeba4b5b"
      },
      "source": [
        "!git clone https://github.com/nlpyang/pytorch-transformers"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'pytorch-transformers'...\n",
            "remote: Enumerating objects: 6469, done.\u001b[K\n",
            "remote: Total 6469 (delta 0), reused 0 (delta 0), pack-reused 6469\u001b[K\n",
            "Receiving objects: 100% (6469/6469), 3.63 MiB | 2.86 MiB/s, done.\n",
            "Resolving deltas: 100% (4633/4633), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9lCBOZzTQkD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b69c863c-6545-4129-fe20-3473dd0749df"
      },
      "source": [
        "%cd pytorch-transformers/examples/lm_finetuning"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/pytorch-transformers/examples/lm_finetuning\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XF7DWETtUq87",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import shutil"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxaXgR3hUvB2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "73ad756d-cbe7-459f-c403-603c5d509ae3"
      },
      "source": [
        "src = directory_path+\"/data_lm.txt\" ##copying newly created data to the same finetuned folder.\n",
        "dst = os.getcwd()\n",
        "shutil.copy(src, dst)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/pytorch-transformers/examples/lm_finetuning/data_lm.txt'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50Hui54ZUxar",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "22c9cbdc-6201-4124-e458-a1021c7c955c"
      },
      "source": [
        "os.listdir(os.getcwd())"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['pregenerate_training_data.py',\n",
              " 'README.md',\n",
              " 'data_lm.txt',\n",
              " 'finetune_on_pregenerated.py',\n",
              " 'simple_lm_finetuning.py']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjVuAOpEUzXE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3dde848f-7a66-4ff1-a21a-d031a488d631"
      },
      "source": [
        "!python3 pregenerate_training_data.py --train_corpus data_lm.txt --bert_model bert-base-uncased --do_lower_case --output_dir training/ --epochs_to_generate 2 --max_seq_len 256"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100% 231508/231508 [00:00<00:00, 320565.37B/s]\n",
            "Loading Dataset: 100000 lines [03:17, 505.95 lines/s]\n",
            "Epoch:   0% 0/2 [00:00<?, ?it/s]\n",
            "Document:   0% 0/50000 [00:00<?, ?it/s]\u001b[A\n",
            "Document:   0% 166/50000 [00:00<00:30, 1657.26it/s]\u001b[A\n",
            "Document:   1% 352/50000 [00:00<00:29, 1711.81it/s]\u001b[A\n",
            "Document:   1% 529/50000 [00:00<00:28, 1727.05it/s]\u001b[A\n",
            "Document:   1% 721/50000 [00:00<00:27, 1780.70it/s]\u001b[A\n",
            "Document:   2% 895/50000 [00:00<00:27, 1768.07it/s]\u001b[A\n",
            "Document:   2% 1089/50000 [00:00<00:26, 1814.56it/s]\u001b[A\n",
            "Document:   3% 1274/50000 [00:00<00:26, 1821.68it/s]\u001b[A\n",
            "Document:   3% 1460/50000 [00:00<00:26, 1831.98it/s]\u001b[A\n",
            "Document:   3% 1634/50000 [00:00<00:26, 1801.86it/s]\u001b[A\n",
            "Document:   4% 1819/50000 [00:01<00:26, 1811.49it/s]\u001b[A\n",
            "Document:   4% 2000/50000 [00:01<00:26, 1809.52it/s]\u001b[A\n",
            "Document:   4% 2189/50000 [00:01<00:26, 1832.60it/s]\u001b[A\n",
            "Document:   5% 2371/50000 [00:01<00:27, 1750.13it/s]\u001b[A\n",
            "Document:   5% 2547/50000 [00:01<00:27, 1747.73it/s]\u001b[A\n",
            "Document:   5% 2722/50000 [00:01<00:27, 1742.51it/s]\u001b[A\n",
            "Document:   6% 2906/50000 [00:01<00:26, 1770.32it/s]\u001b[A\n",
            "Document:   6% 3091/50000 [00:01<00:26, 1792.34it/s]\u001b[A\n",
            "Document:   7% 3279/50000 [00:01<00:25, 1815.22it/s]\u001b[A\n",
            "Document:   7% 3461/50000 [00:01<00:26, 1786.59it/s]\u001b[A\n",
            "Document:   7% 3649/50000 [00:02<00:25, 1812.17it/s]\u001b[A\n",
            "Document:   8% 3832/50000 [00:02<00:25, 1815.29it/s]\u001b[A\n",
            "Document:   8% 4014/50000 [00:02<00:25, 1788.59it/s]\u001b[A\n",
            "Document:   8% 4194/50000 [00:02<00:26, 1751.66it/s]\u001b[A\n",
            "Document:   9% 4370/50000 [00:02<00:26, 1739.46it/s]\u001b[A\n",
            "Document:   9% 4557/50000 [00:02<00:25, 1774.81it/s]\u001b[A\n",
            "Document:  10% 4751/50000 [00:02<00:24, 1820.03it/s]\u001b[A\n",
            "Document:  10% 4937/50000 [00:02<00:24, 1829.74it/s]\u001b[A\n",
            "Document:  10% 5124/50000 [00:02<00:24, 1839.73it/s]\u001b[A\n",
            "Document:  11% 5309/50000 [00:02<00:24, 1831.86it/s]\u001b[A\n",
            "Document:  11% 5500/50000 [00:03<00:24, 1852.56it/s]\u001b[A\n",
            "Document:  11% 5687/50000 [00:03<00:23, 1855.39it/s]\u001b[A\n",
            "Document:  12% 5873/50000 [00:03<00:24, 1798.77it/s]\u001b[A\n",
            "Document:  12% 6054/50000 [00:03<00:24, 1800.85it/s]\u001b[A\n",
            "Document:  12% 6235/50000 [00:03<00:24, 1770.88it/s]\u001b[A\n",
            "Document:  13% 6413/50000 [00:03<00:24, 1771.81it/s]\u001b[A\n",
            "Document:  13% 6597/50000 [00:03<00:24, 1790.14it/s]\u001b[A\n",
            "Document:  14% 6777/50000 [00:03<00:24, 1765.01it/s]\u001b[A\n",
            "Document:  14% 6954/50000 [00:03<00:24, 1754.89it/s]\u001b[A\n",
            "Document:  14% 7135/50000 [00:03<00:24, 1765.46it/s]\u001b[A\n",
            "Document:  15% 7323/50000 [00:04<00:23, 1798.00it/s]\u001b[A\n",
            "Document:  15% 7509/50000 [00:04<00:23, 1815.93it/s]\u001b[A\n",
            "Document:  15% 7693/50000 [00:04<00:23, 1822.26it/s]\u001b[A\n",
            "Document:  16% 7878/50000 [00:04<00:23, 1829.05it/s]\u001b[A\n",
            "Document:  16% 8062/50000 [00:04<00:23, 1777.67it/s]\u001b[A\n",
            "Document:  16% 8241/50000 [00:04<00:23, 1776.86it/s]\u001b[A\n",
            "Document:  17% 8419/50000 [00:04<00:23, 1763.77it/s]\u001b[A\n",
            "Document:  17% 8605/50000 [00:04<00:23, 1789.90it/s]\u001b[A\n",
            "Document:  18% 8785/50000 [00:04<00:23, 1748.91it/s]\u001b[A\n",
            "Document:  18% 8976/50000 [00:04<00:22, 1793.81it/s]\u001b[A\n",
            "Document:  18% 9156/50000 [00:05<00:23, 1766.20it/s]\u001b[A\n",
            "Document:  19% 9357/50000 [00:05<00:22, 1830.85it/s]\u001b[A\n",
            "Document:  19% 9542/50000 [00:05<00:22, 1831.97it/s]\u001b[A\n",
            "Document:  19% 9726/50000 [00:05<00:22, 1814.90it/s]\u001b[A\n",
            "Document:  20% 9908/50000 [00:05<00:22, 1788.91it/s]\u001b[A\n",
            "Document:  20% 10090/50000 [00:05<00:22, 1795.94it/s]\u001b[A\n",
            "Document:  21% 10270/50000 [00:05<00:22, 1759.58it/s]\u001b[A\n",
            "Document:  21% 10456/50000 [00:05<00:22, 1787.52it/s]\u001b[A\n",
            "Document:  21% 10642/50000 [00:05<00:21, 1805.91it/s]\u001b[A\n",
            "Document:  22% 10828/50000 [00:06<00:21, 1819.66it/s]\u001b[A\n",
            "Document:  22% 11011/50000 [00:06<00:21, 1821.57it/s]\u001b[A\n",
            "Document:  22% 11203/50000 [00:06<00:20, 1848.35it/s]\u001b[A\n",
            "Document:  23% 11389/50000 [00:06<00:21, 1767.66it/s]\u001b[A\n",
            "Document:  23% 11567/50000 [00:06<00:21, 1748.53it/s]\u001b[A\n",
            "Document:  23% 11743/50000 [00:06<00:22, 1720.36it/s]\u001b[A\n",
            "Document:  24% 11916/50000 [00:06<00:22, 1709.55it/s]\u001b[A\n",
            "Document:  24% 12101/50000 [00:06<00:21, 1747.46it/s]\u001b[A\n",
            "Document:  25% 12278/50000 [00:06<00:21, 1753.19it/s]\u001b[A\n",
            "Document:  25% 12454/50000 [00:06<00:21, 1749.77it/s]\u001b[A\n",
            "Document:  25% 12643/50000 [00:07<00:20, 1784.85it/s]\u001b[A\n",
            "Document:  26% 12834/50000 [00:07<00:20, 1818.36it/s]\u001b[A\n",
            "Document:  26% 13019/50000 [00:07<00:20, 1825.52it/s]\u001b[A\n",
            "Document:  26% 13202/50000 [00:07<00:20, 1779.09it/s]\u001b[A\n",
            "Document:  27% 13389/50000 [00:07<00:20, 1803.24it/s]\u001b[A\n",
            "Document:  27% 13570/50000 [00:07<00:20, 1758.11it/s]\u001b[A\n",
            "Document:  28% 13753/50000 [00:07<00:20, 1776.98it/s]\u001b[A\n",
            "Document:  28% 13932/50000 [00:07<00:20, 1764.68it/s]\u001b[A\n",
            "Document:  28% 14114/50000 [00:07<00:20, 1778.68it/s]\u001b[A\n",
            "Document:  29% 14301/50000 [00:07<00:19, 1803.28it/s]\u001b[A\n",
            "Document:  29% 14491/50000 [00:08<00:19, 1830.66it/s]\u001b[A\n",
            "Document:  29% 14675/50000 [00:08<00:19, 1814.23it/s]\u001b[A\n",
            "Document:  30% 14865/50000 [00:08<00:19, 1837.90it/s]\u001b[A\n",
            "Document:  30% 15050/50000 [00:08<00:19, 1802.22it/s]\u001b[A\n",
            "Document:  30% 15239/50000 [00:08<00:19, 1826.06it/s]\u001b[A\n",
            "Document:  31% 15422/50000 [00:08<00:19, 1777.97it/s]\u001b[A\n",
            "Document:  31% 15612/50000 [00:08<00:18, 1811.21it/s]\u001b[A\n",
            "Document:  32% 15794/50000 [00:08<00:18, 1805.96it/s]\u001b[A\n",
            "Document:  32% 15975/50000 [00:08<00:18, 1791.00it/s]\u001b[A\n",
            "Document:  32% 16167/50000 [00:09<00:18, 1827.80it/s]\u001b[A\n",
            "Document:  33% 16354/50000 [00:09<00:18, 1839.68it/s]\u001b[A\n",
            "Document:  33% 16541/50000 [00:09<00:18, 1843.56it/s]\u001b[A\n",
            "Document:  33% 16727/50000 [00:09<00:18, 1848.03it/s]\u001b[A\n",
            "Document:  34% 16912/50000 [00:09<00:18, 1802.03it/s]\u001b[A\n",
            "Document:  34% 17102/50000 [00:09<00:17, 1828.34it/s]\u001b[A\n",
            "Document:  35% 17286/50000 [00:09<00:18, 1792.56it/s]\u001b[A\n",
            "Document:  35% 17475/50000 [00:09<00:17, 1819.42it/s]\u001b[A\n",
            "Document:  35% 17658/50000 [00:09<00:17, 1820.05it/s]\u001b[A\n",
            "Document:  36% 17846/50000 [00:09<00:17, 1835.13it/s]\u001b[A\n",
            "Document:  36% 18032/50000 [00:10<00:17, 1842.13it/s]\u001b[A\n",
            "Document:  36% 18217/50000 [00:10<00:17, 1843.94it/s]\u001b[A\n",
            "Document:  37% 18402/50000 [00:10<00:17, 1825.32it/s]\u001b[A\n",
            "Document:  37% 18596/50000 [00:10<00:16, 1858.21it/s]\u001b[A\n",
            "Document:  38% 18783/50000 [00:10<00:17, 1821.92it/s]\u001b[A\n",
            "Document:  38% 18976/50000 [00:10<00:16, 1852.33it/s]\u001b[A\n",
            "Document:  38% 19162/50000 [00:10<00:17, 1805.64it/s]\u001b[A\n",
            "Document:  39% 19344/50000 [00:10<00:17, 1796.12it/s]\u001b[A\n",
            "Document:  39% 19526/50000 [00:10<00:16, 1802.46it/s]\u001b[A\n",
            "Document:  39% 19707/50000 [00:10<00:16, 1802.03it/s]\u001b[A\n",
            "Document:  40% 19896/50000 [00:11<00:16, 1823.66it/s]\u001b[A\n",
            "Document:  40% 20084/50000 [00:11<00:16, 1838.94it/s]\u001b[A\n",
            "Document:  41% 20270/50000 [00:11<00:16, 1840.59it/s]\u001b[A\n",
            "Document:  41% 20455/50000 [00:11<00:16, 1820.81it/s]\u001b[A\n",
            "Document:  41% 20638/50000 [00:11<00:16, 1783.58it/s]\u001b[A\n",
            "Document:  42% 20817/50000 [00:11<00:16, 1780.58it/s]\u001b[A\n",
            "Document:  42% 21005/50000 [00:11<00:16, 1807.17it/s]\u001b[A\n",
            "Document:  42% 21186/50000 [00:11<00:16, 1800.35it/s]\u001b[A\n",
            "Document:  43% 21371/50000 [00:11<00:15, 1814.90it/s]\u001b[A\n",
            "Document:  43% 21553/50000 [00:11<00:15, 1812.36it/s]\u001b[A\n",
            "Document:  43% 21738/50000 [00:12<00:15, 1822.82it/s]\u001b[A\n",
            "Document:  44% 21921/50000 [00:12<00:15, 1806.13it/s]\u001b[A\n",
            "Document:  44% 22115/50000 [00:12<00:15, 1842.42it/s]\u001b[A\n",
            "Document:  45% 22314/50000 [00:12<00:14, 1880.54it/s]\u001b[A\n",
            "Document:  45% 22503/50000 [00:12<00:14, 1855.97it/s]\u001b[A\n",
            "Document:  45% 22689/50000 [00:12<00:15, 1807.10it/s]\u001b[A\n",
            "Document:  46% 22871/50000 [00:12<00:15, 1751.04it/s]\u001b[A\n",
            "Document:  46% 23056/50000 [00:12<00:15, 1777.88it/s]\u001b[A\n",
            "Document:  46% 23239/50000 [00:12<00:14, 1789.79it/s]\u001b[A\n",
            "Document:  47% 23425/50000 [00:12<00:14, 1808.48it/s]\u001b[A\n",
            "Document:  47% 23607/50000 [00:13<00:14, 1790.45it/s]\u001b[A\n",
            "Document:  48% 23787/50000 [00:13<00:14, 1788.35it/s]\u001b[A\n",
            "Document:  48% 23974/50000 [00:13<00:14, 1810.42it/s]\u001b[A\n",
            "Document:  48% 24156/50000 [00:13<00:14, 1812.17it/s]\u001b[A\n",
            "Document:  49% 24338/50000 [00:13<00:15, 1660.12it/s]\u001b[A\n",
            "Document:  49% 24507/50000 [00:13<00:15, 1653.48it/s]\u001b[A\n",
            "Document:  49% 24689/50000 [00:13<00:14, 1699.88it/s]\u001b[A\n",
            "Document:  50% 24874/50000 [00:13<00:14, 1741.63it/s]\u001b[A\n",
            "Document:  50% 25055/50000 [00:13<00:14, 1760.07it/s]\u001b[A\n",
            "Document:  50% 25233/50000 [00:14<00:14, 1734.45it/s]\u001b[A\n",
            "Document:  51% 25409/50000 [00:14<00:14, 1740.90it/s]\u001b[A\n",
            "Document:  51% 25597/50000 [00:14<00:13, 1778.50it/s]\u001b[A\n",
            "Document:  52% 25776/50000 [00:14<00:13, 1772.83it/s]\u001b[A\n",
            "Document:  52% 25959/50000 [00:14<00:13, 1787.01it/s]\u001b[A\n",
            "Document:  52% 26139/50000 [00:14<00:13, 1750.74it/s]\u001b[A\n",
            "Document:  53% 26315/50000 [00:14<00:13, 1696.02it/s]\u001b[A\n",
            "Document:  53% 26496/50000 [00:14<00:13, 1728.64it/s]\u001b[A\n",
            "Document:  53% 26675/50000 [00:14<00:13, 1744.36it/s]\u001b[A\n",
            "Document:  54% 26852/50000 [00:14<00:13, 1749.59it/s]\u001b[A\n",
            "Document:  54% 27029/50000 [00:15<00:13, 1754.70it/s]\u001b[A\n",
            "Document:  54% 27219/50000 [00:15<00:12, 1794.57it/s]\u001b[A\n",
            "Document:  55% 27399/50000 [00:15<00:12, 1780.15it/s]\u001b[A\n",
            "Document:  55% 27584/50000 [00:15<00:12, 1800.23it/s]\u001b[A\n",
            "Document:  56% 27769/50000 [00:15<00:12, 1811.92it/s]\u001b[A\n",
            "Document:  56% 27951/50000 [00:15<00:12, 1813.23it/s]\u001b[A\n",
            "Document:  56% 28133/50000 [00:15<00:12, 1723.94it/s]\u001b[A\n",
            "Document:  57% 28316/50000 [00:15<00:12, 1753.91it/s]\u001b[A\n",
            "Document:  57% 28497/50000 [00:15<00:12, 1768.37it/s]\u001b[A\n",
            "Document:  57% 28676/50000 [00:15<00:12, 1773.77it/s]\u001b[A\n",
            "Document:  58% 28866/50000 [00:16<00:11, 1807.72it/s]\u001b[A\n",
            "Document:  58% 29048/50000 [00:16<00:11, 1790.56it/s]\u001b[A\n",
            "Document:  58% 29228/50000 [00:16<00:11, 1765.26it/s]\u001b[A\n",
            "Document:  59% 29415/50000 [00:16<00:11, 1793.25it/s]\u001b[A\n",
            "Document:  59% 29595/50000 [00:16<00:11, 1774.39it/s]\u001b[A\n",
            "Document:  60% 29773/50000 [00:16<00:11, 1744.53it/s]\u001b[A\n",
            "Document:  60% 29948/50000 [00:16<00:11, 1738.34it/s]\u001b[A\n",
            "Document:  60% 30125/50000 [00:16<00:11, 1747.70it/s]\u001b[A\n",
            "Document:  61% 30300/50000 [00:16<00:11, 1720.48it/s]\u001b[A\n",
            "Document:  61% 30484/50000 [00:17<00:11, 1753.66it/s]\u001b[A\n",
            "Document:  61% 30660/50000 [00:17<00:11, 1680.53it/s]\u001b[A\n",
            "Document:  62% 30837/50000 [00:17<00:11, 1703.78it/s]\u001b[A\n",
            "Document:  62% 31025/50000 [00:17<00:10, 1750.78it/s]\u001b[A\n",
            "Document:  62% 31201/50000 [00:17<00:11, 1694.07it/s]\u001b[A\n",
            "Document:  63% 31382/50000 [00:17<00:10, 1725.48it/s]\u001b[A\n",
            "Document:  63% 31556/50000 [00:17<00:10, 1691.48it/s]\u001b[A\n",
            "Document:  63% 31730/50000 [00:17<00:10, 1703.56it/s]\u001b[A\n",
            "Document:  64% 31915/50000 [00:17<00:10, 1743.06it/s]\u001b[A\n",
            "Document:  64% 32096/50000 [00:17<00:10, 1761.71it/s]\u001b[A\n",
            "Document:  65% 32280/50000 [00:18<00:09, 1784.34it/s]\u001b[A\n",
            "Document:  65% 32464/50000 [00:18<00:09, 1800.37it/s]\u001b[A\n",
            "Document:  65% 32651/50000 [00:18<00:09, 1818.99it/s]\u001b[A\n",
            "Document:  66% 32834/50000 [00:18<00:09, 1803.95it/s]\u001b[A\n",
            "Document:  66% 33019/50000 [00:18<00:09, 1816.76it/s]\u001b[A\n",
            "Document:  66% 33202/50000 [00:18<00:09, 1820.12it/s]\u001b[A\n",
            "Document:  67% 33385/50000 [00:18<00:10, 1632.87it/s]\u001b[A\n",
            "Document:  67% 33563/50000 [00:18<00:09, 1673.31it/s]\u001b[A\n",
            "Document:  67% 33748/50000 [00:18<00:09, 1719.65it/s]\u001b[A\n",
            "Document:  68% 33923/50000 [00:18<00:09, 1698.87it/s]\u001b[A\n",
            "Document:  68% 34105/50000 [00:19<00:09, 1729.95it/s]\u001b[A\n",
            "Document:  69% 34295/50000 [00:19<00:08, 1776.20it/s]\u001b[A\n",
            "Document:  69% 34475/50000 [00:19<00:08, 1780.78it/s]\u001b[A\n",
            "Document:  69% 34656/50000 [00:19<00:08, 1788.25it/s]\u001b[A\n",
            "Document:  70% 34844/50000 [00:19<00:08, 1813.53it/s]\u001b[A\n",
            "Document:  70% 35026/50000 [00:19<00:08, 1793.19it/s]\u001b[A\n",
            "Document:  70% 35206/50000 [00:19<00:08, 1660.33it/s]\u001b[A\n",
            "Document:  71% 35375/50000 [00:19<00:08, 1659.10it/s]\u001b[A\n",
            "Document:  71% 35562/50000 [00:19<00:08, 1716.43it/s]\u001b[A\n",
            "Document:  71% 35736/50000 [00:20<00:08, 1700.51it/s]\u001b[A\n",
            "Document:  72% 35908/50000 [00:20<00:08, 1699.79it/s]\u001b[A\n",
            "Document:  72% 36081/50000 [00:20<00:08, 1705.66it/s]\u001b[A\n",
            "Document:  73% 36269/50000 [00:20<00:07, 1753.16it/s]\u001b[A\n",
            "Document:  73% 36448/50000 [00:20<00:07, 1763.55it/s]\u001b[A\n",
            "Document:  73% 36631/50000 [00:20<00:07, 1780.49it/s]\u001b[A\n",
            "Document:  74% 36816/50000 [00:20<00:07, 1800.59it/s]\u001b[A\n",
            "Document:  74% 36997/50000 [00:20<00:07, 1775.44it/s]\u001b[A\n",
            "Document:  74% 37189/50000 [00:20<00:07, 1814.09it/s]\u001b[A\n",
            "Document:  75% 37371/50000 [00:20<00:07, 1751.61it/s]\u001b[A\n",
            "Document:  75% 37553/50000 [00:21<00:07, 1770.22it/s]\u001b[A\n",
            "Document:  75% 37741/50000 [00:21<00:06, 1800.96it/s]\u001b[A\n",
            "Document:  76% 37923/50000 [00:21<00:06, 1805.20it/s]\u001b[A\n",
            "Document:  76% 38116/50000 [00:21<00:06, 1840.40it/s]\u001b[A\n",
            "Document:  77% 38301/50000 [00:21<00:06, 1820.97it/s]\u001b[A\n",
            "Document:  77% 38484/50000 [00:21<00:06, 1817.92it/s]\u001b[A\n",
            "Document:  77% 38673/50000 [00:21<00:06, 1837.99it/s]\u001b[A\n",
            "Document:  78% 38858/50000 [00:21<00:06, 1782.86it/s]\u001b[A\n",
            "Document:  78% 39037/50000 [00:21<00:06, 1764.61it/s]\u001b[A\n",
            "Document:  78% 39219/50000 [00:21<00:06, 1778.57it/s]\u001b[A\n",
            "Document:  79% 39398/50000 [00:22<00:05, 1774.91it/s]\u001b[A\n",
            "Document:  79% 39576/50000 [00:22<00:05, 1771.58it/s]\u001b[A\n",
            "Document:  80% 39754/50000 [00:22<00:05, 1765.41it/s]\u001b[A\n",
            "Document:  80% 39935/50000 [00:22<00:05, 1776.63it/s]\u001b[A\n",
            "Document:  80% 40120/50000 [00:22<00:05, 1796.35it/s]\u001b[A\n",
            "Document:  81% 40306/50000 [00:22<00:05, 1812.83it/s]\u001b[A\n",
            "Document:  81% 40491/50000 [00:22<00:05, 1823.72it/s]\u001b[A\n",
            "Document:  81% 40674/50000 [00:22<00:05, 1779.84it/s]\u001b[A\n",
            "Document:  82% 40853/50000 [00:22<00:05, 1775.45it/s]\u001b[A\n",
            "Document:  82% 41031/50000 [00:22<00:05, 1774.09it/s]\u001b[A\n",
            "Document:  82% 41220/50000 [00:23<00:04, 1804.73it/s]\u001b[A\n",
            "Document:  83% 41402/50000 [00:23<00:04, 1807.23it/s]\u001b[A\n",
            "Document:  83% 41583/50000 [00:23<00:04, 1785.13it/s]\u001b[A\n",
            "Document:  84% 41762/50000 [00:23<00:04, 1744.33it/s]\u001b[A\n",
            "Document:  84% 41944/50000 [00:23<00:04, 1763.82it/s]\u001b[A\n",
            "Document:  84% 42127/50000 [00:23<00:04, 1781.06it/s]\u001b[A\n",
            "Document:  85% 42306/50000 [00:23<00:04, 1779.76it/s]\u001b[A\n",
            "Document:  85% 42485/50000 [00:23<00:04, 1773.76it/s]\u001b[A\n",
            "Document:  85% 42669/50000 [00:23<00:04, 1791.57it/s]\u001b[A\n",
            "Document:  86% 42851/50000 [00:24<00:03, 1797.40it/s]\u001b[A\n",
            "Document:  86% 43031/50000 [00:24<00:03, 1758.63it/s]\u001b[A\n",
            "Document:  86% 43208/50000 [00:24<00:03, 1719.22it/s]\u001b[A\n",
            "Document:  87% 43387/50000 [00:24<00:03, 1738.01it/s]\u001b[A\n",
            "Document:  87% 43568/50000 [00:24<00:03, 1758.36it/s]\u001b[A\n",
            "Document:  88% 43764/50000 [00:24<00:03, 1811.93it/s]\u001b[A\n",
            "Document:  88% 43946/50000 [00:24<00:03, 1770.00it/s]\u001b[A\n",
            "Document:  88% 44125/50000 [00:24<00:03, 1774.11it/s]\u001b[A\n",
            "Document:  89% 44303/50000 [00:24<00:03, 1718.99it/s]\u001b[A\n",
            "Document:  89% 44481/50000 [00:24<00:03, 1734.51it/s]\u001b[A\n",
            "Document:  89% 44668/50000 [00:25<00:03, 1772.60it/s]\u001b[A\n",
            "Document:  90% 44846/50000 [00:25<00:02, 1771.46it/s]\u001b[A\n",
            "Document:  90% 45034/50000 [00:25<00:02, 1800.72it/s]\u001b[A\n",
            "Document:  90% 45225/50000 [00:25<00:02, 1829.82it/s]\u001b[A\n",
            "Document:  91% 45409/50000 [00:25<00:02, 1818.48it/s]\u001b[A\n",
            "Document:  91% 45592/50000 [00:25<00:02, 1800.34it/s]\u001b[A\n",
            "Document:  92% 45773/50000 [00:25<00:02, 1797.43it/s]\u001b[A\n",
            "Document:  92% 45960/50000 [00:25<00:02, 1817.76it/s]\u001b[A\n",
            "Document:  92% 46142/50000 [00:25<00:02, 1741.71it/s]\u001b[A\n",
            "Document:  93% 46322/50000 [00:25<00:02, 1757.57it/s]\u001b[A\n",
            "Document:  93% 46504/50000 [00:26<00:01, 1773.71it/s]\u001b[A\n",
            "Document:  93% 46698/50000 [00:26<00:01, 1819.53it/s]\u001b[A\n",
            "Document:  94% 46881/50000 [00:26<00:01, 1815.87it/s]\u001b[A\n",
            "Document:  94% 47067/50000 [00:26<00:01, 1828.34it/s]\u001b[A\n",
            "Document:  95% 47251/50000 [00:26<00:01, 1796.43it/s]\u001b[A\n",
            "Document:  95% 47439/50000 [00:26<00:01, 1820.09it/s]\u001b[A\n",
            "Document:  95% 47625/50000 [00:26<00:01, 1831.63it/s]\u001b[A\n",
            "Document:  96% 47813/50000 [00:26<00:01, 1844.63it/s]\u001b[A\n",
            "Document:  96% 47998/50000 [00:26<00:01, 1719.08it/s]\u001b[A\n",
            "Document:  96% 48186/50000 [00:27<00:01, 1763.91it/s]\u001b[A\n",
            "Document:  97% 48377/50000 [00:27<00:00, 1804.16it/s]\u001b[A\n",
            "Document:  97% 48563/50000 [00:27<00:00, 1819.24it/s]\u001b[A\n",
            "Document:  97% 48746/50000 [00:27<00:00, 1814.26it/s]\u001b[A\n",
            "Document:  98% 48935/50000 [00:27<00:00, 1835.60it/s]\u001b[A\n",
            "Document:  98% 49120/50000 [00:27<00:00, 1837.09it/s]\u001b[A\n",
            "Document:  99% 49306/50000 [00:27<00:00, 1841.58it/s]\u001b[A\n",
            "Document:  99% 49491/50000 [00:27<00:00, 1741.45it/s]\u001b[A\n",
            "Document:  99% 49668/50000 [00:27<00:00, 1747.69it/s]\u001b[A\n",
            "Document: 100% 50000/50000 [00:28<00:00, 1784.18it/s]\n",
            "Epoch:  50% 1/2 [00:28<00:28, 28.02s/it]\n",
            "Document:   0% 0/50000 [00:00<?, ?it/s]\u001b[A\n",
            "Document:   0% 183/50000 [00:00<00:27, 1825.61it/s]\u001b[A\n",
            "Document:   1% 368/50000 [00:00<00:27, 1828.51it/s]\u001b[A\n",
            "Document:   1% 550/50000 [00:00<00:27, 1824.71it/s]\u001b[A\n",
            "Document:   1% 734/50000 [00:00<00:26, 1828.82it/s]\u001b[A\n",
            "Document:   2% 912/50000 [00:00<00:27, 1813.83it/s]\u001b[A\n",
            "Document:   2% 1106/50000 [00:00<00:26, 1848.20it/s]\u001b[A\n",
            "Document:   3% 1289/50000 [00:00<00:26, 1841.13it/s]\u001b[A\n",
            "Document:   3% 1459/50000 [00:00<00:27, 1773.65it/s]\u001b[A\n",
            "Document:   3% 1627/50000 [00:00<00:28, 1722.47it/s]\u001b[A\n",
            "Document:   4% 1813/50000 [00:01<00:27, 1760.23it/s]\u001b[A\n",
            "Document:   4% 1988/50000 [00:01<00:27, 1755.74it/s]\u001b[A\n",
            "Document:   4% 2169/50000 [00:01<00:27, 1770.01it/s]\u001b[A\n",
            "Document:   5% 2351/50000 [00:01<00:26, 1781.89it/s]\u001b[A\n",
            "Document:   5% 2535/50000 [00:01<00:26, 1797.69it/s]\u001b[A\n",
            "Document:   5% 2714/50000 [00:01<00:26, 1785.54it/s]\u001b[A\n",
            "Document:   6% 2904/50000 [00:01<00:25, 1816.70it/s]\u001b[A\n",
            "Document:   6% 3090/50000 [00:01<00:25, 1828.37it/s]\u001b[A\n",
            "Document:   7% 3278/50000 [00:01<00:25, 1840.11it/s]\u001b[A\n",
            "Document:   7% 3462/50000 [00:01<00:26, 1727.85it/s]\u001b[A\n",
            "Document:   7% 3637/50000 [00:02<00:26, 1728.64it/s]\u001b[A\n",
            "Document:   8% 3811/50000 [00:02<00:26, 1732.02it/s]\u001b[A\n",
            "Document:   8% 3990/50000 [00:02<00:26, 1748.88it/s]\u001b[A\n",
            "Document:   8% 4169/50000 [00:02<00:26, 1757.98it/s]\u001b[A\n",
            "Document:   9% 4346/50000 [00:02<00:26, 1746.65it/s]\u001b[A\n",
            "Document:   9% 4522/50000 [00:02<00:26, 1748.71it/s]\u001b[A\n",
            "Document:   9% 4703/50000 [00:02<00:25, 1764.92it/s]\u001b[A\n",
            "Document:  10% 4880/50000 [00:02<00:26, 1721.95it/s]\u001b[A\n",
            "Document:  10% 5063/50000 [00:02<00:25, 1752.88it/s]\u001b[A\n",
            "Document:  10% 5239/50000 [00:02<00:26, 1690.13it/s]\u001b[A\n",
            "Document:  11% 5420/50000 [00:03<00:25, 1723.58it/s]\u001b[A\n",
            "Document:  11% 5601/50000 [00:03<00:25, 1745.92it/s]\u001b[A\n",
            "Document:  12% 5782/50000 [00:03<00:25, 1762.32it/s]\u001b[A\n",
            "Document:  12% 5959/50000 [00:03<00:25, 1733.25it/s]\u001b[A\n",
            "Document:  12% 6148/50000 [00:03<00:24, 1773.71it/s]\u001b[A\n",
            "Document:  13% 6326/50000 [00:03<00:25, 1742.49it/s]\u001b[A\n",
            "Document:  13% 6514/50000 [00:03<00:24, 1780.41it/s]\u001b[A\n",
            "Document:  13% 6699/50000 [00:03<00:24, 1796.48it/s]\u001b[A\n",
            "Document:  14% 6880/50000 [00:03<00:24, 1753.65it/s]\u001b[A\n",
            "Document:  14% 7060/50000 [00:03<00:24, 1764.94it/s]\u001b[A\n",
            "Document:  14% 7248/50000 [00:04<00:23, 1797.75it/s]\u001b[A\n",
            "Document:  15% 7429/50000 [00:04<00:23, 1778.03it/s]\u001b[A\n",
            "Document:  15% 7614/50000 [00:04<00:23, 1798.59it/s]\u001b[A\n",
            "Document:  16% 7795/50000 [00:04<00:23, 1800.79it/s]\u001b[A\n",
            "Document:  16% 7976/50000 [00:04<00:23, 1787.58it/s]\u001b[A\n",
            "Document:  16% 8158/50000 [00:04<00:23, 1796.35it/s]\u001b[A\n",
            "Document:  17% 8338/50000 [00:04<00:23, 1782.01it/s]\u001b[A\n",
            "Document:  17% 8519/50000 [00:04<00:23, 1789.92it/s]\u001b[A\n",
            "Document:  17% 8699/50000 [00:04<00:23, 1753.81it/s]\u001b[A\n",
            "Document:  18% 8875/50000 [00:05<00:23, 1734.87it/s]\u001b[A\n",
            "Document:  18% 9051/50000 [00:05<00:23, 1740.41it/s]\u001b[A\n",
            "Document:  18% 9235/50000 [00:05<00:23, 1767.60it/s]\u001b[A\n",
            "Document:  19% 9426/50000 [00:05<00:22, 1805.90it/s]\u001b[A\n",
            "Document:  19% 9618/50000 [00:05<00:21, 1838.66it/s]\u001b[A\n",
            "Document:  20% 9803/50000 [00:05<00:21, 1837.55it/s]\u001b[A\n",
            "Document:  20% 9990/50000 [00:05<00:21, 1843.73it/s]\u001b[A\n",
            "Document:  20% 10175/50000 [00:05<00:21, 1834.22it/s]\u001b[A\n",
            "Document:  21% 10359/50000 [00:05<00:21, 1820.56it/s]\u001b[A\n",
            "Document:  21% 10542/50000 [00:05<00:21, 1821.19it/s]\u001b[A\n",
            "Document:  21% 10725/50000 [00:06<00:25, 1529.98it/s]\u001b[A\n",
            "Document:  22% 10895/50000 [00:06<00:24, 1575.88it/s]\u001b[A\n",
            "Document:  22% 11076/50000 [00:06<00:23, 1639.42it/s]\u001b[A\n",
            "Document:  23% 11268/50000 [00:06<00:22, 1714.21it/s]\u001b[A\n",
            "Document:  23% 11444/50000 [00:06<00:22, 1685.51it/s]\u001b[A\n",
            "Document:  23% 11616/50000 [00:06<00:22, 1687.01it/s]\u001b[A\n",
            "Document:  24% 11796/50000 [00:06<00:22, 1717.14it/s]\u001b[A\n",
            "Document:  24% 11974/50000 [00:06<00:21, 1733.59it/s]\u001b[A\n",
            "Document:  24% 12149/50000 [00:06<00:21, 1726.17it/s]\u001b[A\n",
            "Document:  25% 12323/50000 [00:07<00:22, 1687.14it/s]\u001b[A\n",
            "Document:  25% 12493/50000 [00:07<00:23, 1622.83it/s]\u001b[A\n",
            "Document:  25% 12657/50000 [00:07<00:22, 1625.73it/s]\u001b[A\n",
            "Document:  26% 12832/50000 [00:07<00:22, 1660.00it/s]\u001b[A\n",
            "Document:  26% 13008/50000 [00:07<00:21, 1687.88it/s]\u001b[A\n",
            "Document:  26% 13180/50000 [00:07<00:21, 1696.80it/s]\u001b[A\n",
            "Document:  27% 13356/50000 [00:07<00:21, 1713.26it/s]\u001b[A\n",
            "Document:  27% 13528/50000 [00:07<00:21, 1702.44it/s]\u001b[A\n",
            "Document:  27% 13699/50000 [00:07<00:21, 1697.70it/s]\u001b[A\n",
            "Document:  28% 13878/50000 [00:07<00:20, 1724.14it/s]\u001b[A\n",
            "Document:  28% 14051/50000 [00:08<00:21, 1696.78it/s]\u001b[A\n",
            "Document:  28% 14221/50000 [00:08<00:21, 1651.13it/s]\u001b[A\n",
            "Document:  29% 14399/50000 [00:08<00:21, 1683.70it/s]\u001b[A\n",
            "Document:  29% 14568/50000 [00:08<00:21, 1676.87it/s]\u001b[A\n",
            "Document:  29% 14737/50000 [00:08<00:21, 1676.98it/s]\u001b[A\n",
            "Document:  30% 14920/50000 [00:08<00:20, 1718.19it/s]\u001b[A\n",
            "Document:  30% 15095/50000 [00:08<00:20, 1727.33it/s]\u001b[A\n",
            "Document:  31% 15280/50000 [00:08<00:19, 1760.85it/s]\u001b[A\n",
            "Document:  31% 15460/50000 [00:08<00:19, 1769.67it/s]\u001b[A\n",
            "Document:  31% 15638/50000 [00:08<00:19, 1745.40it/s]\u001b[A\n",
            "Document:  32% 15815/50000 [00:09<00:19, 1750.87it/s]\u001b[A\n",
            "Document:  32% 15991/50000 [00:09<00:19, 1711.15it/s]\u001b[A\n",
            "Document:  32% 16176/50000 [00:09<00:19, 1749.88it/s]\u001b[A\n",
            "Document:  33% 16365/50000 [00:09<00:18, 1787.93it/s]\u001b[A\n",
            "Document:  33% 16545/50000 [00:09<00:18, 1768.72it/s]\u001b[A\n",
            "Document:  33% 16726/50000 [00:09<00:18, 1780.87it/s]\u001b[A\n",
            "Document:  34% 16911/50000 [00:09<00:18, 1799.06it/s]\u001b[A\n",
            "Document:  34% 17099/50000 [00:09<00:18, 1820.99it/s]\u001b[A\n",
            "Document:  35% 17282/50000 [00:09<00:18, 1808.96it/s]\u001b[A\n",
            "Document:  35% 17464/50000 [00:09<00:18, 1801.92it/s]\u001b[A\n",
            "Document:  35% 17645/50000 [00:10<00:17, 1800.74it/s]\u001b[A\n",
            "Document:  36% 17826/50000 [00:10<00:18, 1722.61it/s]\u001b[A\n",
            "Document:  36% 18012/50000 [00:10<00:18, 1760.47it/s]\u001b[A\n",
            "Document:  36% 18199/50000 [00:10<00:17, 1789.92it/s]\u001b[A\n",
            "Document:  37% 18379/50000 [00:10<00:18, 1709.59it/s]\u001b[A\n",
            "Document:  37% 18563/50000 [00:10<00:18, 1744.15it/s]\u001b[A\n",
            "Document:  38% 18753/50000 [00:10<00:17, 1788.12it/s]\u001b[A\n",
            "Document:  38% 18939/50000 [00:10<00:17, 1809.03it/s]\u001b[A\n",
            "Document:  38% 19127/50000 [00:10<00:16, 1829.56it/s]\u001b[A\n",
            "Document:  39% 19311/50000 [00:11<00:16, 1806.93it/s]\u001b[A\n",
            "Document:  39% 19493/50000 [00:11<00:17, 1706.69it/s]\u001b[A\n",
            "Document:  39% 19674/50000 [00:11<00:17, 1733.33it/s]\u001b[A\n",
            "Document:  40% 19868/50000 [00:11<00:16, 1790.46it/s]\u001b[A\n",
            "Document:  40% 20054/50000 [00:11<00:16, 1809.97it/s]\u001b[A\n",
            "Document:  40% 20243/50000 [00:11<00:16, 1832.09it/s]\u001b[A\n",
            "Document:  41% 20427/50000 [00:11<00:16, 1826.62it/s]\u001b[A\n",
            "Document:  41% 20615/50000 [00:11<00:15, 1842.30it/s]\u001b[A\n",
            "Document:  42% 20800/50000 [00:11<00:16, 1758.76it/s]\u001b[A\n",
            "Document:  42% 20983/50000 [00:11<00:16, 1775.88it/s]\u001b[A\n",
            "Document:  42% 21162/50000 [00:12<00:16, 1755.28it/s]\u001b[A\n",
            "Document:  43% 21339/50000 [00:12<00:16, 1692.70it/s]\u001b[A\n",
            "Document:  43% 21510/50000 [00:12<00:17, 1671.79it/s]\u001b[A\n",
            "Document:  43% 21692/50000 [00:12<00:16, 1712.52it/s]\u001b[A\n",
            "Document:  44% 21883/50000 [00:12<00:15, 1766.03it/s]\u001b[A\n",
            "Document:  44% 22075/50000 [00:12<00:15, 1808.70it/s]\u001b[A\n",
            "Document:  45% 22270/50000 [00:12<00:15, 1847.30it/s]\u001b[A\n",
            "Document:  45% 22465/50000 [00:12<00:14, 1876.45it/s]\u001b[A\n",
            "Document:  45% 22654/50000 [00:12<00:14, 1867.01it/s]\u001b[A\n",
            "Document:  46% 22844/50000 [00:12<00:14, 1872.94it/s]\u001b[A\n",
            "Document:  46% 23032/50000 [00:13<00:14, 1860.15it/s]\u001b[A\n",
            "Document:  46% 23219/50000 [00:13<00:15, 1781.45it/s]\u001b[A\n",
            "Document:  47% 23410/50000 [00:13<00:14, 1814.96it/s]\u001b[A\n",
            "Document:  47% 23594/50000 [00:13<00:14, 1820.32it/s]\u001b[A\n",
            "Document:  48% 23787/50000 [00:13<00:14, 1851.26it/s]\u001b[A\n",
            "Document:  48% 23973/50000 [00:13<00:14, 1839.85it/s]\u001b[A\n",
            "Document:  48% 24162/50000 [00:13<00:13, 1853.83it/s]\u001b[A\n",
            "Document:  49% 24348/50000 [00:13<00:13, 1855.33it/s]\u001b[A\n",
            "Document:  49% 24537/50000 [00:13<00:13, 1864.87it/s]\u001b[A\n",
            "Document:  49% 24724/50000 [00:14<00:13, 1839.70it/s]\u001b[A\n",
            "Document:  50% 24913/50000 [00:14<00:13, 1853.96it/s]\u001b[A\n",
            "Document:  50% 25099/50000 [00:14<00:13, 1781.50it/s]\u001b[A\n",
            "Document:  51% 25296/50000 [00:14<00:13, 1832.01it/s]\u001b[A\n",
            "Document:  51% 25482/50000 [00:14<00:13, 1837.82it/s]\u001b[A\n",
            "Document:  51% 25667/50000 [00:14<00:13, 1829.19it/s]\u001b[A\n",
            "Document:  52% 25851/50000 [00:14<00:13, 1828.63it/s]\u001b[A\n",
            "Document:  52% 26040/50000 [00:14<00:12, 1846.36it/s]\u001b[A\n",
            "Document:  52% 26225/50000 [00:14<00:13, 1827.75it/s]\u001b[A\n",
            "Document:  53% 26413/50000 [00:14<00:12, 1842.14it/s]\u001b[A\n",
            "Document:  53% 26606/50000 [00:15<00:12, 1865.99it/s]\u001b[A\n",
            "Document:  54% 26800/50000 [00:15<00:12, 1884.11it/s]\u001b[A\n",
            "Document:  54% 26989/50000 [00:15<00:12, 1798.14it/s]\u001b[A\n",
            "Document:  54% 27174/50000 [00:15<00:12, 1811.57it/s]\u001b[A\n",
            "Document:  55% 27359/50000 [00:15<00:12, 1821.64it/s]\u001b[A\n",
            "Document:  55% 27553/50000 [00:15<00:12, 1854.14it/s]\u001b[A\n",
            "Document:  55% 27739/50000 [00:15<00:12, 1828.41it/s]\u001b[A\n",
            "Document:  56% 27925/50000 [00:15<00:12, 1836.71it/s]\u001b[A\n",
            "Document:  56% 28113/50000 [00:15<00:11, 1848.91it/s]\u001b[A\n",
            "Document:  57% 28309/50000 [00:15<00:11, 1879.53it/s]\u001b[A\n",
            "Document:  57% 28498/50000 [00:16<00:11, 1817.16it/s]\u001b[A\n",
            "Document:  57% 28687/50000 [00:16<00:11, 1834.03it/s]\u001b[A\n",
            "Document:  58% 28871/50000 [00:16<00:11, 1812.34it/s]\u001b[A\n",
            "Document:  58% 29065/50000 [00:16<00:11, 1844.98it/s]\u001b[A\n",
            "Document:  58% 29250/50000 [00:16<00:11, 1817.18it/s]\u001b[A\n",
            "Document:  59% 29434/50000 [00:16<00:11, 1822.14it/s]\u001b[A\n",
            "Document:  59% 29617/50000 [00:16<00:11, 1812.08it/s]\u001b[A\n",
            "Document:  60% 29812/50000 [00:16<00:10, 1849.01it/s]\u001b[A\n",
            "Document:  60% 29998/50000 [00:16<00:10, 1844.73it/s]\u001b[A\n",
            "Document:  60% 30188/50000 [00:16<00:10, 1860.26it/s]\u001b[A\n",
            "Document:  61% 30375/50000 [00:17<00:10, 1796.43it/s]\u001b[A\n",
            "Document:  61% 30567/50000 [00:17<00:10, 1830.56it/s]\u001b[A\n",
            "Document:  62% 30751/50000 [00:17<00:10, 1785.78it/s]\u001b[A\n",
            "Document:  62% 30939/50000 [00:17<00:10, 1810.95it/s]\u001b[A\n",
            "Document:  62% 31123/50000 [00:17<00:10, 1818.13it/s]\u001b[A\n",
            "Document:  63% 31306/50000 [00:17<00:10, 1821.19it/s]\u001b[A\n",
            "Document:  63% 31489/50000 [00:17<00:10, 1820.17it/s]\u001b[A\n",
            "Document:  63% 31672/50000 [00:17<00:10, 1817.68it/s]\u001b[A\n",
            "Document:  64% 31859/50000 [00:17<00:09, 1832.61it/s]\u001b[A\n",
            "Document:  64% 32043/50000 [00:17<00:09, 1830.46it/s]\u001b[A\n",
            "Document:  64% 32227/50000 [00:18<00:09, 1811.20it/s]\u001b[A\n",
            "Document:  65% 32409/50000 [00:18<00:09, 1779.59it/s]\u001b[A\n",
            "Document:  65% 32588/50000 [00:18<00:10, 1737.52it/s]\u001b[A\n",
            "Document:  66% 32776/50000 [00:18<00:09, 1775.77it/s]\u001b[A\n",
            "Document:  66% 32955/50000 [00:18<00:09, 1763.44it/s]\u001b[A\n",
            "Document:  66% 33132/50000 [00:18<00:09, 1754.36it/s]\u001b[A\n",
            "Document:  67% 33311/50000 [00:18<00:09, 1764.29it/s]\u001b[A\n",
            "Document:  67% 33494/50000 [00:18<00:09, 1783.33it/s]\u001b[A\n",
            "Document:  67% 33673/50000 [00:18<00:09, 1767.95it/s]\u001b[A\n",
            "Document:  68% 33857/50000 [00:19<00:09, 1787.41it/s]\u001b[A\n",
            "Document:  68% 34047/50000 [00:19<00:08, 1817.93it/s]\u001b[A\n",
            "Document:  68% 34230/50000 [00:19<00:09, 1731.74it/s]\u001b[A\n",
            "Document:  69% 34405/50000 [00:19<00:09, 1696.78it/s]\u001b[A\n",
            "Document:  69% 34596/50000 [00:19<00:08, 1753.55it/s]\u001b[A\n",
            "Document:  70% 34780/50000 [00:19<00:08, 1778.58it/s]\u001b[A\n",
            "Document:  70% 34961/50000 [00:19<00:08, 1787.82it/s]\u001b[A\n",
            "Document:  70% 35141/50000 [00:19<00:08, 1791.09it/s]\u001b[A\n",
            "Document:  71% 35328/50000 [00:19<00:08, 1811.77it/s]\u001b[A\n",
            "Document:  71% 35521/50000 [00:19<00:07, 1842.82it/s]\u001b[A\n",
            "Document:  71% 35706/50000 [00:20<00:07, 1804.90it/s]\u001b[A\n",
            "Document:  72% 35887/50000 [00:20<00:07, 1791.30it/s]\u001b[A\n",
            "Document:  72% 36067/50000 [00:20<00:07, 1789.54it/s]\u001b[A\n",
            "Document:  72% 36247/50000 [00:20<00:07, 1750.89it/s]\u001b[A\n",
            "Document:  73% 36423/50000 [00:20<00:07, 1747.82it/s]\u001b[A\n",
            "Document:  73% 36599/50000 [00:20<00:07, 1749.39it/s]\u001b[A\n",
            "Document:  74% 36775/50000 [00:20<00:07, 1742.80it/s]\u001b[A\n",
            "Document:  74% 36950/50000 [00:20<00:07, 1743.92it/s]\u001b[A\n",
            "Document:  74% 37142/50000 [00:20<00:07, 1789.23it/s]\u001b[A\n",
            "Document:  75% 37330/50000 [00:20<00:06, 1813.16it/s]\u001b[A\n",
            "Document:  75% 37512/50000 [00:21<00:06, 1803.52it/s]\u001b[A\n",
            "Document:  75% 37693/50000 [00:21<00:07, 1757.24it/s]\u001b[A\n",
            "Document:  76% 37870/50000 [00:21<00:07, 1617.08it/s]\u001b[A\n",
            "Document:  76% 38035/50000 [00:21<00:07, 1565.86it/s]\u001b[A\n",
            "Document:  76% 38221/50000 [00:21<00:07, 1641.42it/s]\u001b[A\n",
            "Document:  77% 38402/50000 [00:21<00:06, 1687.45it/s]\u001b[A\n",
            "Document:  77% 38587/50000 [00:21<00:06, 1732.53it/s]\u001b[A\n",
            "Document:  78% 38779/50000 [00:21<00:06, 1784.34it/s]\u001b[A\n",
            "Document:  78% 38967/50000 [00:21<00:06, 1811.78it/s]\u001b[A\n",
            "Document:  78% 39150/50000 [00:22<00:06, 1775.29it/s]\u001b[A\n",
            "Document:  79% 39329/50000 [00:22<00:06, 1752.18it/s]\u001b[A\n",
            "Document:  79% 39511/50000 [00:22<00:05, 1769.85it/s]\u001b[A\n",
            "Document:  79% 39701/50000 [00:22<00:05, 1806.74it/s]\u001b[A\n",
            "Document:  80% 39883/50000 [00:22<00:05, 1744.57it/s]\u001b[A\n",
            "Document:  80% 40066/50000 [00:22<00:05, 1767.51it/s]\u001b[A\n",
            "Document:  80% 40248/50000 [00:22<00:05, 1780.30it/s]\u001b[A\n",
            "Document:  81% 40429/50000 [00:22<00:05, 1788.46it/s]\u001b[A\n",
            "Document:  81% 40616/50000 [00:22<00:05, 1811.69it/s]\u001b[A\n",
            "Document:  82% 40804/50000 [00:22<00:05, 1831.36it/s]\u001b[A\n",
            "Document:  82% 40988/50000 [00:23<00:05, 1798.21it/s]\u001b[A\n",
            "Document:  82% 41169/50000 [00:23<00:04, 1795.33it/s]\u001b[A\n",
            "Document:  83% 41349/50000 [00:23<00:04, 1778.22it/s]\u001b[A\n",
            "Document:  83% 41530/50000 [00:23<00:04, 1786.04it/s]\u001b[A\n",
            "Document:  83% 41709/50000 [00:23<00:04, 1703.50it/s]\u001b[A\n",
            "Document:  84% 41892/50000 [00:23<00:04, 1738.83it/s]\u001b[A\n",
            "Document:  84% 42080/50000 [00:23<00:04, 1777.50it/s]\u001b[A\n",
            "Document:  85% 42268/50000 [00:23<00:04, 1802.30it/s]\u001b[A\n",
            "Document:  85% 42456/50000 [00:23<00:04, 1824.70it/s]\u001b[A\n",
            "Document:  85% 42640/50000 [00:23<00:04, 1828.98it/s]\u001b[A\n",
            "Document:  86% 42824/50000 [00:24<00:03, 1803.49it/s]\u001b[A\n",
            "Document:  86% 43008/50000 [00:24<00:03, 1813.62it/s]\u001b[A\n",
            "Document:  86% 43193/50000 [00:24<00:03, 1822.96it/s]\u001b[A\n",
            "Document:  87% 43376/50000 [00:24<00:03, 1798.92it/s]\u001b[A\n",
            "Document:  87% 43557/50000 [00:24<00:03, 1723.57it/s]\u001b[A\n",
            "Document:  87% 43731/50000 [00:24<00:03, 1714.53it/s]\u001b[A\n",
            "Document:  88% 43904/50000 [00:24<00:03, 1719.08it/s]\u001b[A\n",
            "Document:  88% 44086/50000 [00:24<00:03, 1745.09it/s]\u001b[A\n",
            "Document:  89% 44269/50000 [00:24<00:03, 1767.29it/s]\u001b[A\n",
            "Document:  89% 44460/50000 [00:25<00:03, 1806.72it/s]\u001b[A\n",
            "Document:  89% 44646/50000 [00:25<00:02, 1822.04it/s]\u001b[A\n",
            "Document:  90% 44835/50000 [00:25<00:02, 1841.39it/s]\u001b[A\n",
            "Document:  90% 45024/50000 [00:25<00:02, 1855.54it/s]\u001b[A\n",
            "Document:  90% 45210/50000 [00:25<00:02, 1850.68it/s]\u001b[A\n",
            "Document:  91% 45396/50000 [00:25<00:02, 1749.09it/s]\u001b[A\n",
            "Document:  91% 45579/50000 [00:25<00:02, 1771.31it/s]\u001b[A\n",
            "Document:  92% 45767/50000 [00:25<00:02, 1799.63it/s]\u001b[A\n",
            "Document:  92% 45954/50000 [00:25<00:02, 1819.81it/s]\u001b[A\n",
            "Document:  92% 46142/50000 [00:25<00:02, 1836.42it/s]\u001b[A\n",
            "Document:  93% 46338/50000 [00:26<00:01, 1871.21it/s]\u001b[A\n",
            "Document:  93% 46526/50000 [00:26<00:01, 1866.28it/s]\u001b[A\n",
            "Document:  93% 46718/50000 [00:26<00:01, 1879.45it/s]\u001b[A\n",
            "Document:  94% 46907/50000 [00:26<00:01, 1834.97it/s]\u001b[A\n",
            "Document:  94% 47091/50000 [00:26<00:01, 1597.45it/s]\u001b[A\n",
            "Document:  95% 47266/50000 [00:26<00:01, 1639.14it/s]\u001b[A\n",
            "Document:  95% 47447/50000 [00:26<00:01, 1686.30it/s]\u001b[A\n",
            "Document:  95% 47642/50000 [00:26<00:01, 1755.21it/s]\u001b[A\n",
            "Document:  96% 47824/50000 [00:26<00:01, 1772.56it/s]\u001b[A\n",
            "Document:  96% 48008/50000 [00:27<00:01, 1788.60it/s]\u001b[A\n",
            "Document:  96% 48189/50000 [00:27<00:01, 1724.48it/s]\u001b[A\n",
            "Document:  97% 48364/50000 [00:27<00:00, 1706.91it/s]\u001b[A\n",
            "Document:  97% 48548/50000 [00:27<00:00, 1742.43it/s]\u001b[A\n",
            "Document:  97% 48736/50000 [00:27<00:00, 1778.97it/s]\u001b[A\n",
            "Document:  98% 48915/50000 [00:27<00:00, 1752.11it/s]\u001b[A\n",
            "Document:  98% 49092/50000 [00:27<00:00, 1752.91it/s]\u001b[A\n",
            "Document:  99% 49268/50000 [00:27<00:00, 1714.59it/s]\u001b[A\n",
            "Document:  99% 49449/50000 [00:27<00:00, 1737.86it/s]\u001b[A\n",
            "Document:  99% 49626/50000 [00:27<00:00, 1746.43it/s]\u001b[A\n",
            "Document: 100% 49812/50000 [00:28<00:00, 1778.26it/s]\u001b[A\n",
            "Document: 100% 50000/50000 [00:28<00:00, 1776.61it/s]\n",
            "Epoch: 100% 2/2 [00:56<00:00, 28.08s/it]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqllmPVRZ4wT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "966b14b9-b9d7-4472-ecc8-a95b8f713de7"
      },
      "source": [
        "os.listdir(os.getcwd())"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['pregenerate_training_data.py',\n",
              " 'README.md',\n",
              " 'data_lm.txt',\n",
              " 'finetune_on_pregenerated.py',\n",
              " 'simple_lm_finetuning.py',\n",
              " 'training']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D14tGbirZ6nz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 974
        },
        "outputId": "d2bd341e-47aa-42dd-c56b-80ad6ae60732"
      },
      "source": [
        "!python3 finetune_on_pregenerated.py --pregenerated_data training/ --bert_model bert-base-uncased --do_lower_case --train_batch_size 16  --output_dir finetuned_lm/ --epochs 2"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-04-21 15:53:15,970: device: cuda n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "2020-04-21 15:53:16,736: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "2020-04-21 15:53:17,529: https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json not found in cache or force_download set to True, downloading to /tmp/tmptm3rqlpg\n",
            "100% 361/361 [00:00<00:00, 277722.62B/s]\n",
            "2020-04-21 15:53:18,289: copying /tmp/tmptm3rqlpg to cache at /root/.cache/torch/pytorch_transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685\n",
            "2020-04-21 15:53:18,290: creating metadata file for /root/.cache/torch/pytorch_transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685\n",
            "2020-04-21 15:53:18,290: removing temp file /tmp/tmptm3rqlpg\n",
            "2020-04-21 15:53:18,290: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/pytorch_transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685\n",
            "2020-04-21 15:53:18,291: Model config {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "2020-04-21 15:53:19,042: https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin not found in cache or force_download set to True, downloading to /tmp/tmprq5ref0l\n",
            "100% 440473133/440473133 [00:35<00:00, 12444125.13B/s]\n",
            "2020-04-21 15:53:55,209: copying /tmp/tmprq5ref0l to cache at /root/.cache/torch/pytorch_transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "2020-04-21 15:53:56,620: creating metadata file for /root/.cache/torch/pytorch_transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "2020-04-21 15:53:56,620: removing temp file /tmp/tmprq5ref0l\n",
            "2020-04-21 15:53:56,687: loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/pytorch_transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "2020-04-21 15:54:08,927: ***** Running training *****\n",
            "2020-04-21 15:54:08,928:   Num examples = 100000\n",
            "2020-04-21 15:54:08,928:   Batch size = 16\n",
            "2020-04-21 15:54:08,928:   Num steps = 6250\n",
            "2020-04-21 15:54:08,960: Loading training examples for epoch 0\n",
            "Training examples: 100% 50000/50000 [00:18<00:00, 2705.11it/s]\n",
            "2020-04-21 15:54:27,444: Loading complete!\n",
            "Epoch 0: 100% 3125/3125 [1:36:32<00:00,  1.85s/it, Loss: 1.91454]\n",
            "2020-04-21 17:31:00,464: Loading training examples for epoch 1\n",
            "Training examples: 100% 50000/50000 [00:19<00:00, 2591.47it/s]\n",
            "2020-04-21 17:31:19,759: Loading complete!\n",
            "Epoch 1: 100% 3125/3125 [1:36:27<00:00,  1.85s/it, Loss: 1.80956]\n",
            "2020-04-21 19:07:47,300: ** ** * Saving fine-tuned model ** ** * \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5kBxWmpU4c4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "np.random.seed(56)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7GtPqgYWU1f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df.review = train_df.review.str.lower()\n",
        "sentences = train_df.review.values\n",
        "\n",
        "# We need to add special tokens at the beginning and end of each sentence for BERT to work properly\n",
        "sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\n",
        "labels = train_df.sentiment.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7DewJu3WaN0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "outputId": "8033b7e6-53f8-4208-b42d-e3105fc72717"
      },
      "source": [
        "%%time\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
        "print (\"Tokenize the first sentence:\")\n",
        "print (tokenized_texts[0])"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokenize the first sentence:\n",
            "['[CLS]', 'with', 'all', 'this', 'stuff', 'going', 'down', 'at', 'the', 'moment', 'with', 'm', '##j', 'i', \"'\", 've', 'started', 'listening', 'to', 'his', 'music', ',', 'watching', 'the', 'odd', 'documentary', 'here', 'and', 'there', ',', 'watched', 'the', 'wi', '##z', 'and', 'watched', 'moon', '##walker', 'again', '.', 'maybe', 'i', 'just', 'want', 'to', 'get', 'a', 'certain', 'insight', 'into', 'this', 'guy', 'who', 'i', 'thought', 'was', 'really', 'cool', 'in', 'the', 'eighties', 'just', 'to', 'maybe', 'make', 'up', 'my', 'mind', 'whether', 'he', 'is', 'guilty', 'or', 'innocent', '.', 'moon', '##walker', 'is', 'part', 'biography', ',', 'part', 'feature', 'film', 'which', 'i', 'remember', 'going', 'to', 'see', 'at', 'the', 'cinema', 'when', 'it', 'was', 'originally', 'released', '.', 'some', 'of', 'it', 'has', 'subtle', 'messages', 'about', 'm', '##j', \"'\", 's', 'feeling', 'towards', 'the', 'press', 'and', 'also', 'the', 'obvious', 'message', 'of', 'drugs', 'are', 'bad', 'm', \"'\", 'kay', '.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'visually', 'impressive', 'but', 'of', 'course', 'this', 'is', 'all', 'about', 'michael', 'jackson', 'so', 'unless', 'you', 'remotely', 'like', 'm', '##j', 'in', 'anyway', 'then', 'you', 'are', 'going', 'to', 'hate', 'this', 'and', 'find', 'it', 'boring', '.', 'some', 'may', 'call', 'm', '##j', 'an', 'ego', '##tist', 'for', 'consent', '##ing', 'to', 'the', 'making', 'of', 'this', 'movie', 'but', 'm', '##j', 'and', 'most', 'of', 'his', 'fans', 'would', 'say', 'that', 'he', 'made', 'it', 'for', 'the', 'fans', 'which', 'if', 'true', 'is', 'really', 'nice', 'of', 'him', '.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'the', 'actual', 'feature', 'film', 'bit', 'when', 'it', 'finally', 'starts', 'is', 'only', 'on', 'for', '20', 'minutes', 'or', 'so', 'excluding', 'the', 'smooth', 'criminal', 'sequence', 'and', 'joe', 'pe', '##sc', '##i', 'is', 'convincing', 'as', 'a', 'psycho', '##pathic', 'all', 'powerful', 'drug', 'lord', '.', 'why', 'he', 'wants', 'm', '##j', 'dead', 'so', 'bad', 'is', 'beyond', 'me', '.', 'because', 'm', '##j', 'overheard', 'his', 'plans', '?', 'nah', ',', 'joe', 'pe', '##sc', '##i', \"'\", 's', 'character', 'ran', '##ted', 'that', 'he', 'wanted', 'people', 'to', 'know', 'it', 'is', 'he', 'who', 'is', 'supplying', 'drugs', 'etc', 'so', 'i', 'dunn', '##o', ',', 'maybe', 'he', 'just', 'hates', 'm', '##j', \"'\", 's', 'music', '.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'lots', 'of', 'cool', 'things', 'in', 'this', 'like', 'm', '##j', 'turning', 'into', 'a', 'car', 'and', 'a', 'robot', 'and', 'the', 'whole', 'speed', 'demon', 'sequence', '.', 'also', ',', 'the', 'director', 'must', 'have', 'had', 'the', 'patience', 'of', 'a', 'saint', 'when', 'it', 'came', 'to', 'filming', 'the', 'kidd', '##y', 'bad', 'sequence', 'as', 'usually', 'directors', 'hate', 'working', 'with', 'one', 'kid', 'let', 'alone', 'a', 'whole', 'bunch', 'of', 'them', 'performing', 'a', 'complex', 'dance', 'scene', '.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'bottom', 'line', ',', 'this', 'movie', 'is', 'for', 'people', 'who', 'like', 'm', '##j', 'on', 'one', 'level', 'or', 'another', '(', 'which', 'i', 'think', 'is', 'most', 'people', ')', '.', 'if', 'not', ',', 'then', 'stay', 'away', '.', 'it', 'does', 'try', 'and', 'give', 'off', 'a', 'whole', '##some', 'message', 'and', 'ironically', 'm', '##j', \"'\", 's', 'best', '##est', 'buddy', 'in', 'this', 'movie', 'is', 'a', 'girl', '!', 'michael', 'jackson', 'is', 'truly', 'one', 'of', 'the', 'most', 'talented', 'people', 'ever', 'to', 'grace', 'this', 'planet', 'but', 'is', 'he', 'guilty', '?', 'well', ',', 'with', 'all', 'the', 'attention', 'i', \"'\", 've', 'gave', 'this', 'subject', '.', '.', '.', '.', 'hmm', '##m', 'well', 'i', 'don', \"'\", 't', 'know', 'because', 'people', 'can', 'be', 'different', 'behind', 'closed', 'doors', ',', 'i', 'know', 'this', 'for', 'a', 'fact', '.', 'he', 'is', 'either', 'an', 'extremely', 'nice', 'but', 'stupid', 'guy', 'or', 'one', 'of', 'the', 'most', 'sick', '##est', 'liar', '##s', '.', 'i', 'hope', 'he', 'is', 'not', 'the', 'latter', '.', '[SEP]']\n",
            "CPU times: user 1min 39s, sys: 1.1 ms, total: 1min 39s\n",
            "Wall time: 1min 40s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcJkMic9Wd9v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "11968cd8-dfda-43de-d5a5-27d0df634969"
      },
      "source": [
        "len(tokenized_texts[0])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "544"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jla4cJeGW4lT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#input_ids=[]\n",
        "#for i in tqdm_notebook(range(len(tokenized_texts))):\n",
        "#    input_ids.append(tokenizer.convert_tokens_to_ids(tokenized_texts[i]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ddpuk0ZOW78c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LEN = 256\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ak_7ebnkXjt1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Create attention masks\n",
        "attention_masks = []\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in input_ids:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  attention_masks.append(seq_mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1RxRZwkXl_Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use train_test_split to split our data into train and validation sets for training\n",
        "\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels,random_state=56, test_size=0.2)\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,random_state=56, test_size=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2CYlM9bXtA3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Convert all of our data into torch tensors, the required datatype for our model\n",
        "\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "train_labels = torch.tensor(train_labels)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ieBoX-ioXv7O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 16\n",
        "\n",
        "# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n",
        "# with an iterator the entire dataset does not need to be loaded into memory\n",
        "\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1BJZAVeXyLw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "65b6ea0b-a439-4ef5-f2fb-d9226c94fb31"
      },
      "source": [
        "## Define model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# device ='cpu'\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased',num_labels=2)\n",
        "model.to(device)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ui47wanIX_Cf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr = 2e-5\n",
        "max_grad_norm = 1.0\n",
        "num_total_steps = 1000\n",
        "num_warmup_steps = 100\n",
        "warmup_proportion = float(num_warmup_steps) / float(num_total_steps)  # 0.1\n",
        "\n",
        "\n",
        "### In PyTorch-Transformers, optimizer and schedules are splitted and instantiated like this:\n",
        "optimizer = AdamW(model.parameters(), lr=lr, correct_bias=False)  # To reproduce BertAdam specific behavior set correct_bias=False\n",
        "scheduler = WarmupLinearSchedule(optimizer, warmup_steps=num_warmup_steps, t_total=num_total_steps)  # PyTorch scheduler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHsBFU8HYEvT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "39275407-0513-4510-a816-975a94ba2aab"
      },
      "source": [
        "len(train_dataloader)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1250"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7EiJVqwYYGeQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 986,
          "referenced_widgets": [
            "5c223babb82c419891ae62977539e5a0",
            "865dde423edc4753ab00bdfdd637a4d6",
            "f1cb5884a9394f96b4dfaf5432e33e91",
            "d88c92e58d6342a9a9e4ab715af37fda",
            "fb1aa164af2e4f2fbc397a6ed2433e80",
            "56f8df7d9bd14e589a67bf544973b3ef",
            "1ddbf3766907471fb2b093869bdd4bc8",
            "c07cc6940cd948d6ac9b462f2b9006fe"
          ]
        },
        "outputId": "a48fb07c-a91f-49c0-afc5-763abeb0e958"
      },
      "source": [
        "total_step = len(train_dataloader)\n",
        "\n",
        "# Store our loss and accuracy for plotting\n",
        "train_loss_set = []\n",
        "\n",
        "\n",
        "epochs = 2\n",
        "\n",
        "# trange is a tqdm wrapper around the normal python range\n",
        "for epoch in tqdm_notebook(range(epochs)):\n",
        "  \n",
        "  \n",
        "\n",
        "    # Training\n",
        "    # Set our model to training mode (as opposed to evaluation mode)\n",
        "    model.train()\n",
        "\n",
        "    # Tracking variables\n",
        "    tr_loss = 0\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\n",
        "\n",
        "    # Train the data for one epoch\n",
        "    for i, batch in enumerate(train_dataloader):\n",
        "      # Add batch to GPU\n",
        "      batch = tuple(t.to(device) for t in batch)\n",
        "      # Unpack the inputs from our dataloader\n",
        "      b_input_ids, b_input_mask, b_labels = batch\n",
        "      # Forward pass\n",
        "      outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "      loss = outputs[0]\n",
        "      train_loss_set.append(loss.item())    \n",
        "      # Backward pass\n",
        "      loss.backward()\n",
        "      # Update parameters and take a step using the computed gradient\n",
        "      optimizer.step()\n",
        "      scheduler.step()\n",
        "      optimizer.zero_grad()\n",
        "      if (i) % 50 == 0:\n",
        "        print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
        "                   .format(epoch+1, epochs, i+1, total_step, loss.item()))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5c223babb82c419891ae62977539e5a0",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=2), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/2], Step [1/1250], Loss: 0.7331\n",
            "Epoch [1/2], Step [51/1250], Loss: 0.4831\n",
            "Epoch [1/2], Step [101/1250], Loss: 0.4891\n",
            "Epoch [1/2], Step [151/1250], Loss: 0.1419\n",
            "Epoch [1/2], Step [201/1250], Loss: 0.5278\n",
            "Epoch [1/2], Step [251/1250], Loss: 0.1509\n",
            "Epoch [1/2], Step [301/1250], Loss: 0.3423\n",
            "Epoch [1/2], Step [351/1250], Loss: 0.4561\n",
            "Epoch [1/2], Step [401/1250], Loss: 0.3110\n",
            "Epoch [1/2], Step [451/1250], Loss: 0.2509\n",
            "Epoch [1/2], Step [501/1250], Loss: 0.1571\n",
            "Epoch [1/2], Step [551/1250], Loss: 0.4639\n",
            "Epoch [1/2], Step [601/1250], Loss: 0.1883\n",
            "Epoch [1/2], Step [651/1250], Loss: 0.4569\n",
            "Epoch [1/2], Step [701/1250], Loss: 0.3622\n",
            "Epoch [1/2], Step [751/1250], Loss: 0.0796\n",
            "Epoch [1/2], Step [801/1250], Loss: 0.1444\n",
            "Epoch [1/2], Step [851/1250], Loss: 0.2723\n",
            "Epoch [1/2], Step [901/1250], Loss: 0.3073\n",
            "Epoch [1/2], Step [951/1250], Loss: 0.4086\n",
            "Epoch [1/2], Step [1001/1250], Loss: 0.2239\n",
            "Epoch [1/2], Step [1051/1250], Loss: 0.4783\n",
            "Epoch [1/2], Step [1101/1250], Loss: 0.5487\n",
            "Epoch [1/2], Step [1151/1250], Loss: 0.2119\n",
            "Epoch [1/2], Step [1201/1250], Loss: 0.0558\n",
            "Epoch [2/2], Step [1/1250], Loss: 0.1318\n",
            "Epoch [2/2], Step [51/1250], Loss: 0.0499\n",
            "Epoch [2/2], Step [101/1250], Loss: 0.0765\n",
            "Epoch [2/2], Step [151/1250], Loss: 0.0793\n",
            "Epoch [2/2], Step [201/1250], Loss: 0.0685\n",
            "Epoch [2/2], Step [251/1250], Loss: 0.1075\n",
            "Epoch [2/2], Step [301/1250], Loss: 0.2142\n",
            "Epoch [2/2], Step [351/1250], Loss: 0.2245\n",
            "Epoch [2/2], Step [401/1250], Loss: 0.1050\n",
            "Epoch [2/2], Step [451/1250], Loss: 0.0441\n",
            "Epoch [2/2], Step [501/1250], Loss: 0.4730\n",
            "Epoch [2/2], Step [551/1250], Loss: 0.1996\n",
            "Epoch [2/2], Step [601/1250], Loss: 0.1591\n",
            "Epoch [2/2], Step [651/1250], Loss: 0.2569\n",
            "Epoch [2/2], Step [701/1250], Loss: 0.1481\n",
            "Epoch [2/2], Step [751/1250], Loss: 0.3187\n",
            "Epoch [2/2], Step [801/1250], Loss: 0.1896\n",
            "Epoch [2/2], Step [851/1250], Loss: 0.2285\n",
            "Epoch [2/2], Step [901/1250], Loss: 0.1156\n",
            "Epoch [2/2], Step [951/1250], Loss: 0.3617\n",
            "Epoch [2/2], Step [1001/1250], Loss: 0.0821\n",
            "Epoch [2/2], Step [1051/1250], Loss: 0.1544\n",
            "Epoch [2/2], Step [1101/1250], Loss: 0.0549\n",
            "Epoch [2/2], Step [1151/1250], Loss: 0.3910\n",
            "Epoch [2/2], Step [1201/1250], Loss: 0.4926\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXU0qEc4YJEB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(model.state_dict(), directory_path+'/model_without_language_model.ckpt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKef2aO0YOCU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Test the model\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for i, batch in enumerate(validation_dataloader):\n",
        "      batch = tuple(t.to(device) for t in batch)\n",
        "      # Unpack the inputs from our dataloader\n",
        "      b_input_ids, b_input_mask, b_labels = batch\n",
        "      # Forward pass\n",
        "      outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "      # print (outputs)\n",
        "      prediction = torch.argmax(outputs[0],dim=1)\n",
        "      total += b_labels.size(0)\n",
        "      correct+=(prediction==b_labels).sum().item()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-f7n77OHYPrm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d0bfc176-29bc-4e3d-c1d6-67f5d74464bc"
      },
      "source": [
        "print('Test Accuracy of the model on val data is: {} %'.format(100 * correct / total))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy of the model on val data is: 90.58 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EX1iGdtBYYVc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#using finetuned language model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WCe4DhaY66p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "063d17b0-8cf5-4b6b-f9cf-029d4d88f65c"
      },
      "source": [
        "\n",
        "directory_path+\"/pytorch-transformers/examples/lm_finetuning/finetuned_lm\""
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/BERT/pytorch-transformers/examples/lm_finetuning/finetuned_lm'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ebSmZuPY87o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_path=\"/content/pytorch-transformers/examples/lm_finetuning/finetuned_lm/\" ## model is stored at this directory."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmVGix5RY_2v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "outputId": "969a30e7-e688-44ab-f3b7-e7d64a051811"
      },
      "source": [
        "%%time\n",
        "tokenizer = BertTokenizer.from_pretrained('/content/pytorch-transformers/examples/lm_finetuning/finetuned_lm')\n",
        "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
        "print (\"Tokenize the first sentence:\")\n",
        "print (tokenized_texts[0])"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokenize the first sentence:\n",
            "['[CLS]', 'with', 'all', 'this', 'stuff', 'going', 'down', 'at', 'the', 'moment', 'with', 'm', '##j', 'i', \"'\", 've', 'started', 'listening', 'to', 'his', 'music', ',', 'watching', 'the', 'odd', 'documentary', 'here', 'and', 'there', ',', 'watched', 'the', 'wi', '##z', 'and', 'watched', 'moon', '##walker', 'again', '.', 'maybe', 'i', 'just', 'want', 'to', 'get', 'a', 'certain', 'insight', 'into', 'this', 'guy', 'who', 'i', 'thought', 'was', 'really', 'cool', 'in', 'the', 'eighties', 'just', 'to', 'maybe', 'make', 'up', 'my', 'mind', 'whether', 'he', 'is', 'guilty', 'or', 'innocent', '.', 'moon', '##walker', 'is', 'part', 'biography', ',', 'part', 'feature', 'film', 'which', 'i', 'remember', 'going', 'to', 'see', 'at', 'the', 'cinema', 'when', 'it', 'was', 'originally', 'released', '.', 'some', 'of', 'it', 'has', 'subtle', 'messages', 'about', 'm', '##j', \"'\", 's', 'feeling', 'towards', 'the', 'press', 'and', 'also', 'the', 'obvious', 'message', 'of', 'drugs', 'are', 'bad', 'm', \"'\", 'kay', '.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'visually', 'impressive', 'but', 'of', 'course', 'this', 'is', 'all', 'about', 'michael', 'jackson', 'so', 'unless', 'you', 'remotely', 'like', 'm', '##j', 'in', 'anyway', 'then', 'you', 'are', 'going', 'to', 'hate', 'this', 'and', 'find', 'it', 'boring', '.', 'some', 'may', 'call', 'm', '##j', 'an', 'ego', '##tist', 'for', 'consent', '##ing', 'to', 'the', 'making', 'of', 'this', 'movie', 'but', 'm', '##j', 'and', 'most', 'of', 'his', 'fans', 'would', 'say', 'that', 'he', 'made', 'it', 'for', 'the', 'fans', 'which', 'if', 'true', 'is', 'really', 'nice', 'of', 'him', '.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'the', 'actual', 'feature', 'film', 'bit', 'when', 'it', 'finally', 'starts', 'is', 'only', 'on', 'for', '20', 'minutes', 'or', 'so', 'excluding', 'the', 'smooth', 'criminal', 'sequence', 'and', 'joe', 'pe', '##sc', '##i', 'is', 'convincing', 'as', 'a', 'psycho', '##pathic', 'all', 'powerful', 'drug', 'lord', '.', 'why', 'he', 'wants', 'm', '##j', 'dead', 'so', 'bad', 'is', 'beyond', 'me', '.', 'because', 'm', '##j', 'overheard', 'his', 'plans', '?', 'nah', ',', 'joe', 'pe', '##sc', '##i', \"'\", 's', 'character', 'ran', '##ted', 'that', 'he', 'wanted', 'people', 'to', 'know', 'it', 'is', 'he', 'who', 'is', 'supplying', 'drugs', 'etc', 'so', 'i', 'dunn', '##o', ',', 'maybe', 'he', 'just', 'hates', 'm', '##j', \"'\", 's', 'music', '.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'lots', 'of', 'cool', 'things', 'in', 'this', 'like', 'm', '##j', 'turning', 'into', 'a', 'car', 'and', 'a', 'robot', 'and', 'the', 'whole', 'speed', 'demon', 'sequence', '.', 'also', ',', 'the', 'director', 'must', 'have', 'had', 'the', 'patience', 'of', 'a', 'saint', 'when', 'it', 'came', 'to', 'filming', 'the', 'kidd', '##y', 'bad', 'sequence', 'as', 'usually', 'directors', 'hate', 'working', 'with', 'one', 'kid', 'let', 'alone', 'a', 'whole', 'bunch', 'of', 'them', 'performing', 'a', 'complex', 'dance', 'scene', '.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'bottom', 'line', ',', 'this', 'movie', 'is', 'for', 'people', 'who', 'like', 'm', '##j', 'on', 'one', 'level', 'or', 'another', '(', 'which', 'i', 'think', 'is', 'most', 'people', ')', '.', 'if', 'not', ',', 'then', 'stay', 'away', '.', 'it', 'does', 'try', 'and', 'give', 'off', 'a', 'whole', '##some', 'message', 'and', 'ironically', 'm', '##j', \"'\", 's', 'best', '##est', 'buddy', 'in', 'this', 'movie', 'is', 'a', 'girl', '!', 'michael', 'jackson', 'is', 'truly', 'one', 'of', 'the', 'most', 'talented', 'people', 'ever', 'to', 'grace', 'this', 'planet', 'but', 'is', 'he', 'guilty', '?', 'well', ',', 'with', 'all', 'the', 'attention', 'i', \"'\", 've', 'gave', 'this', 'subject', '.', '.', '.', '.', 'hmm', '##m', 'well', 'i', 'don', \"'\", 't', 'know', 'because', 'people', 'can', 'be', 'different', 'behind', 'closed', 'doors', ',', 'i', 'know', 'this', 'for', 'a', 'fact', '.', 'he', 'is', 'either', 'an', 'extremely', 'nice', 'but', 'stupid', 'guy', 'or', 'one', 'of', 'the', 'most', 'sick', '##est', 'liar', '##s', '.', 'i', 'hope', 'he', 'is', 'not', 'the', 'latter', '.', '[SEP]']\n",
            "CPU times: user 1min 35s, sys: 319 ms, total: 1min 36s\n",
            "Wall time: 1min 36s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHhIGKB2ZCsu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#input_ids=[]\n",
        "#for i in tqdm_notebook(range(len(tokenized_texts))):\n",
        "#    input_ids.append(tokenizer.convert_tokens_to_ids(tokenized_texts[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-yxCqGnfZG89",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LEN = 256\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yOgXnYGZIbu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in input_ids:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  attention_masks.append(seq_mask)\n",
        "\n",
        "# Use train_test_split to split our data into train and validation sets for training\n",
        "\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels,random_state=56, test_size=0.2)\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,random_state=56, test_size=0.2)\n",
        "\n",
        "#Convert all of our data into torch tensors, the required datatype for our model\n",
        "\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "train_labels = torch.tensor(train_labels)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)\n",
        "\n",
        "batch_size = 16\n",
        "\n",
        "# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n",
        "# with an iterator the entire dataset does not need to be loaded into memory\n",
        "\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zs6b2zXFZMc9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0b63c1b7-16c7-44c4-b0f2-5dd5aac845e8"
      },
      "source": [
        "## Define model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# device ='cpu'\n",
        "model = BertForSequenceClassification.from_pretrained('/content/pytorch-transformers/examples/lm_finetuning/finetuned_lm',num_labels=2)\n",
        "model.to(device)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBXc8FlrZPEk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr = 2e-5\n",
        "max_grad_norm = 1.0\n",
        "num_total_steps = 1000\n",
        "num_warmup_steps = 100\n",
        "warmup_proportion = float(num_warmup_steps) / float(num_total_steps)  # 0.1\n",
        "\n",
        "\n",
        "### In PyTorch-Transformers, optimizer and schedules are splitted and instantiated like this:\n",
        "optimizer = AdamW(model.parameters(), lr=lr, correct_bias=False)  # To reproduce BertAdam specific behavior set correct_bias=False\n",
        "scheduler = WarmupLinearSchedule(optimizer, warmup_steps=num_warmup_steps, t_total=num_total_steps)  # PyTorch scheduler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otN1hF5-ZQL4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 986,
          "referenced_widgets": [
            "72f98b0d5a8049bc8b853d68a8ecd5cb",
            "25af0ec5032543ac871e9122d17d9109",
            "268fa4763409439190e4b9a8612ef0ef",
            "851691febd674ff2932c55a8ee2314b3",
            "05579fce55d6481486c9b979a2fe1b87",
            "c31eee78b2544081a5e77ab8eb7e0cf3",
            "23e2d51cc8a342379ae764661472a692",
            "ac9489f02f2d481f830312ff0059afe3"
          ]
        },
        "outputId": "c1fc584d-900a-45b7-be57-e35ea6b6310a"
      },
      "source": [
        "total_step = len(train_dataloader)\n",
        "\n",
        "# Store our loss and accuracy for plotting\n",
        "train_loss_set = []\n",
        "\n",
        "# Number of training epochs (authors recommend between 2 and 4)\n",
        "epochs = 2\n",
        "\n",
        "# trange is a tqdm wrapper around the normal python range\n",
        "for epoch in tqdm_notebook(range(epochs)):\n",
        "  \n",
        "  \n",
        "\n",
        "    # Training\n",
        "    # Set our model to training mode (as opposed to evaluation mode)\n",
        "    model.train()\n",
        "\n",
        "    # Tracking variables\n",
        "    tr_loss = 0\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\n",
        "\n",
        "    # Train the data for one epoch\n",
        "    for i, batch in enumerate(train_dataloader):\n",
        "      # Add batch to GPU\n",
        "      batch = tuple(t.to(device) for t in batch)\n",
        "      # Unpack the inputs from our dataloader\n",
        "      b_input_ids, b_input_mask, b_labels = batch\n",
        "      # Forward pass\n",
        "      outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "      loss = outputs[0]\n",
        "      train_loss_set.append(loss.item())    \n",
        "      # Backward pass\n",
        "      loss.backward()\n",
        "      # Update parameters and take a step using the computed gradient\n",
        "      optimizer.step()\n",
        "      scheduler.step()\n",
        "      optimizer.zero_grad()\n",
        "      if (i) % 50 == 0:\n",
        "        print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
        "                   .format(epoch+1, epochs, i+1, total_step, loss.item()))\n",
        "\n",
        "torch.save(model.state_dict(), directory_path+'/model_with_language_model.ckpt')"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "72f98b0d5a8049bc8b853d68a8ecd5cb",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=2), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/2], Step [1/1250], Loss: 0.7442\n",
            "Epoch [1/2], Step [51/1250], Loss: 0.2133\n",
            "Epoch [1/2], Step [101/1250], Loss: 0.3433\n",
            "Epoch [1/2], Step [151/1250], Loss: 0.3815\n",
            "Epoch [1/2], Step [201/1250], Loss: 0.4029\n",
            "Epoch [1/2], Step [251/1250], Loss: 0.3777\n",
            "Epoch [1/2], Step [301/1250], Loss: 0.1618\n",
            "Epoch [1/2], Step [351/1250], Loss: 0.1333\n",
            "Epoch [1/2], Step [401/1250], Loss: 0.4292\n",
            "Epoch [1/2], Step [451/1250], Loss: 0.1491\n",
            "Epoch [1/2], Step [501/1250], Loss: 0.3792\n",
            "Epoch [1/2], Step [551/1250], Loss: 0.1495\n",
            "Epoch [1/2], Step [601/1250], Loss: 0.2765\n",
            "Epoch [1/2], Step [651/1250], Loss: 0.3093\n",
            "Epoch [1/2], Step [701/1250], Loss: 0.2580\n",
            "Epoch [1/2], Step [751/1250], Loss: 0.1952\n",
            "Epoch [1/2], Step [801/1250], Loss: 0.2126\n",
            "Epoch [1/2], Step [851/1250], Loss: 0.3025\n",
            "Epoch [1/2], Step [901/1250], Loss: 0.1549\n",
            "Epoch [1/2], Step [951/1250], Loss: 0.2361\n",
            "Epoch [1/2], Step [1001/1250], Loss: 0.2465\n",
            "Epoch [1/2], Step [1051/1250], Loss: 0.2377\n",
            "Epoch [1/2], Step [1101/1250], Loss: 0.3480\n",
            "Epoch [1/2], Step [1151/1250], Loss: 0.4111\n",
            "Epoch [1/2], Step [1201/1250], Loss: 0.1882\n",
            "Epoch [2/2], Step [1/1250], Loss: 0.0418\n",
            "Epoch [2/2], Step [51/1250], Loss: 0.2080\n",
            "Epoch [2/2], Step [101/1250], Loss: 0.0912\n",
            "Epoch [2/2], Step [151/1250], Loss: 0.0863\n",
            "Epoch [2/2], Step [201/1250], Loss: 0.0785\n",
            "Epoch [2/2], Step [251/1250], Loss: 0.2314\n",
            "Epoch [2/2], Step [301/1250], Loss: 0.3318\n",
            "Epoch [2/2], Step [351/1250], Loss: 0.2438\n",
            "Epoch [2/2], Step [401/1250], Loss: 0.0574\n",
            "Epoch [2/2], Step [451/1250], Loss: 0.1586\n",
            "Epoch [2/2], Step [501/1250], Loss: 0.1288\n",
            "Epoch [2/2], Step [551/1250], Loss: 0.1512\n",
            "Epoch [2/2], Step [601/1250], Loss: 0.0560\n",
            "Epoch [2/2], Step [651/1250], Loss: 0.0758\n",
            "Epoch [2/2], Step [701/1250], Loss: 0.0671\n",
            "Epoch [2/2], Step [751/1250], Loss: 0.2800\n",
            "Epoch [2/2], Step [801/1250], Loss: 0.1705\n",
            "Epoch [2/2], Step [851/1250], Loss: 0.2328\n",
            "Epoch [2/2], Step [901/1250], Loss: 0.2477\n",
            "Epoch [2/2], Step [951/1250], Loss: 0.0255\n",
            "Epoch [2/2], Step [1001/1250], Loss: 0.1031\n",
            "Epoch [2/2], Step [1051/1250], Loss: 0.0889\n",
            "Epoch [2/2], Step [1101/1250], Loss: 0.0855\n",
            "Epoch [2/2], Step [1151/1250], Loss: 0.0531\n",
            "Epoch [2/2], Step [1201/1250], Loss: 0.2019\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1QIbDORZS6i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(model.state_dict(), directory_path+'/model_with_language_model.ckpt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPLYnOr5ZVnh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Test the model\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for i, batch in enumerate(validation_dataloader):\n",
        "      batch = tuple(t.to(device) for t in batch)\n",
        "      # Unpack the inputs from our dataloader\n",
        "      b_input_ids, b_input_mask, b_labels = batch\n",
        "      # Forward pass\n",
        "      outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "      # print (outputs)\n",
        "      prediction = torch.argmax(outputs[0],dim=1)\n",
        "      total += b_labels.size(0)\n",
        "      correct+=(prediction==b_labels).sum().item()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WH4JtR-aZXX6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ad356c2d-4acb-4847-b518-4e12150014cf"
      },
      "source": [
        "print('Test Accuracy of the model on val data is: {} %'.format(100 * correct / total))"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy of the model on val data is: 90.8 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CirAm52SwWYw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}